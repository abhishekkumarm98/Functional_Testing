{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd CIFAR-10/ResNet-18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 3159,
     "status": "ok",
     "timestamp": 1656507683429,
     "user": {
      "displayName": "Abhishek kumar Mishra",
      "userId": "13515131134791977528"
     },
     "user_tz": 240
    },
    "id": "KXMJB6isiRyk"
   },
   "outputs": [],
   "source": [
    "import numpy as np, joblib\n",
    "import sys, os, random\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle, gzip\n",
    "from tqdm import tqdm,tqdm_notebook\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1656507683430,
     "user": {
      "displayName": "Abhishek kumar Mishra",
      "userId": "13515131134791977528"
     },
     "user_tz": 240
    },
    "id": "0ok6X5PjkKFX"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "seed_num = 1000\n",
    "\n",
    "# For reproducibility when you run the file with .py\n",
    "torch.cuda.is_available()\n",
    "torch.manual_seed(seed_num)\n",
    "torch.cuda.manual_seed(seed_num)\n",
    "np.random.seed(seed_num)\n",
    "random.seed(seed_num)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "torch.backends.cudnn.deterministic =True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7600,
     "status": "ok",
     "timestamp": 1656507691020,
     "user": {
      "displayName": "Abhishek kumar Mishra",
      "userId": "13515131134791977528"
     },
     "user_tz": 240
    },
    "id": "10agOYLDkidx",
    "outputId": "1cb92267-d171-40d4-eb74-dd636016b981"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(seed_num)\n",
    "torch.cuda.manual_seed(seed_num)\n",
    "np.random.seed(seed_num)\n",
    "random.seed(seed_num)\n",
    "\n",
    "# Normalization\n",
    "train_transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))]) \n",
    "test_transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
    "\n",
    "# Splitting the training and test datasets\n",
    "train_data = datasets.CIFAR10(os.getcwd(), train=True,\n",
    "                              download=True, transform=train_transform)\n",
    "test_data = datasets.CIFAR10(os.getcwd(), train=False,\n",
    "                             download=True, transform=test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1656507691022,
     "user": {
      "displayName": "Abhishek kumar Mishra",
      "userId": "13515131134791977528"
     },
     "user_tz": 240
    },
    "id": "5GnCuQ91lCty"
   },
   "outputs": [],
   "source": [
    "# Split the training set indices into training and validation set indices using 80:20 ratio\n",
    "np.random.seed(seed_num)\n",
    "len_trainset = len(train_data)\n",
    "index_list = list(range(len_trainset))\n",
    "np.random.shuffle(index_list)\n",
    "split_index = 40000\n",
    "train_indices, valid_indices =  index_list[:split_index], index_list[split_index:]\n",
    "\n",
    "# Creating Samplers for training and validation set using the indices\n",
    "np.random.seed(seed_num)\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(valid_indices)\n",
    "\n",
    "torch.manual_seed(seed_num)\n",
    "\n",
    "train_iterator = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\n",
    "val_iterator = DataLoader(train_data, batch_size=batch_size, sampler=valid_sampler)\n",
    "test_iterator = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1656507691023,
     "user": {
      "displayName": "Abhishek kumar Mishra",
      "userId": "13515131134791977528"
     },
     "user_tz": 240
    },
    "id": "hNkLu8CWSUmj"
   },
   "outputs": [],
   "source": [
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "e4fe238396d44cb58e4d2a359e162e55",
      "1344a746f0a048b49330bfee4223e435",
      "da0bb411a7fa4d0aac1333cd9ea1b3e9",
      "d32ce48a5a324ba280933f9084d3683a",
      "5916b897377e46329fba3cc7c18f73a8",
      "9560a152fd3e4d849e775d9874a4b7d7",
      "b184f9fe6eb94bef90cbf673609bdc25",
      "92995c4a616043a2a0051788bd0b6dec",
      "2b8cd6af10e94ffb96cbf9fbc83d0d27",
      "cb761cf8573a492bb9f01913f0f46af3",
      "61b4c885e6aa46efa9e0608ecf3c989b"
     ]
    },
    "executionInfo": {
     "elapsed": 13748,
     "status": "ok",
     "timestamp": 1656507704760,
     "user": {
      "displayName": "Abhishek kumar Mishra",
      "userId": "13515131134791977528"
     },
     "user_tz": 240
    },
    "id": "4D-XvGNzlJn_",
    "outputId": "85b855f3-096d-4faf-e99d-e0b71816ad2c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4fe238396d44cb58e4d2a359e162e55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/44.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ResNet-18 model\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        base = models.resnet18(pretrained=True)\n",
    "        self.base = nn.Sequential(*list(base.children())[:-1])\n",
    "        in_features = base.fc.in_features\n",
    "        self.drop = nn.Dropout()\n",
    "        self.final = nn.Linear(in_features,10)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.base(x)\n",
    "        x = self.drop(x.view(-1,self.final.in_features))\n",
    "        return self.final(x)\n",
    "    \n",
    "model = Model().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 68,
     "status": "ok",
     "timestamp": 1656507704765,
     "user": {
      "displayName": "Abhishek kumar Mishra",
      "userId": "13515131134791977528"
     },
     "user_tz": 240
    },
    "id": "W-97MR9BlJrW",
    "outputId": "3fd06a8d-2257-4e9a-d5d5-7fc4bd51908b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights done !\n",
      "Model:\n",
      " Model(\n",
      "  (base): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (4): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (6): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (7): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  )\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (final): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Loading the weights of ternary model \n",
    "model = torch.load(\"Main_ResNet18_cifar10_Quant.pt\").cuda()\n",
    "print(\"Loading weights done !\")\n",
    "\n",
    "# Summary\n",
    "print(\"Model:\\n\",model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 59,
     "status": "ok",
     "timestamp": 1656507704767,
     "user": {
      "displayName": "Abhishek kumar Mishra",
      "userId": "13515131134791977528"
     },
     "user_tz": 240
    },
    "id": "A8PBc3NYlJuX",
    "outputId": "bc345897-38a3-45e6-c643-b51df60e44b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['base.0.weight', 'base.1.bias', 'base.1.weight', 'base.4.0.conv1.weight', 'base.4.0.bn1.bias', 'base.4.0.bn1.weight', 'base.4.0.conv2.weight', 'base.4.0.bn2.bias', 'base.4.0.bn2.weight', 'base.4.1.conv1.weight', 'base.4.1.bn1.bias', 'base.4.1.bn1.weight', 'base.4.1.conv2.weight', 'base.4.1.bn2.bias', 'base.4.1.bn2.weight', 'base.5.0.conv1.weight', 'base.5.0.bn1.bias', 'base.5.0.bn1.weight', 'base.5.0.conv2.weight', 'base.5.0.bn2.bias', 'base.5.0.bn2.weight', 'base.5.0.downsample.0.weight', 'base.5.0.downsample.1.bias', 'base.5.0.downsample.1.weight', 'base.5.1.conv1.weight', 'base.5.1.bn1.bias', 'base.5.1.bn1.weight', 'base.5.1.conv2.weight', 'base.5.1.bn2.bias', 'base.5.1.bn2.weight', 'base.6.0.conv1.weight', 'base.6.0.bn1.bias', 'base.6.0.bn1.weight', 'base.6.0.conv2.weight', 'base.6.0.bn2.bias', 'base.6.0.bn2.weight', 'base.6.0.downsample.0.weight', 'base.6.0.downsample.1.bias', 'base.6.0.downsample.1.weight', 'base.6.1.conv1.weight', 'base.6.1.bn1.bias', 'base.6.1.bn1.weight', 'base.6.1.conv2.weight', 'base.6.1.bn2.bias', 'base.6.1.bn2.weight', 'base.7.0.conv1.weight', 'base.7.0.bn1.bias', 'base.7.0.bn1.weight', 'base.7.0.conv2.weight', 'base.7.0.bn2.bias', 'base.7.0.bn2.weight', 'base.7.0.downsample.0.weight', 'base.7.0.downsample.1.bias', 'base.7.0.downsample.1.weight', 'base.7.1.conv1.weight', 'base.7.1.bn1.bias', 'base.7.1.bn1.weight', 'base.7.1.conv2.weight', 'base.7.1.bn2.bias', 'base.7.1.bn2.weight', 'final.bias', 'final.weight']\n"
     ]
    }
   ],
   "source": [
    "# Layer names\n",
    "layer_name = [n for n, p in model.named_parameters()]\n",
    "print(layer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1656507704770,
     "user": {
      "displayName": "Abhishek kumar Mishra",
      "userId": "13515131134791977528"
     },
     "user_tz": 240
    },
    "id": "7V_swsrplJxP",
    "outputId": "f110f1a8-d1e2-43d5-b7b9-85bd2f0bee9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base.0.weight tensor(1449, device='cuda:0')\n",
      "base.1.bias tensor(52, device='cuda:0')\n",
      "base.1.weight tensor(52, device='cuda:0')\n",
      "base.4.0.conv1.weight tensor(1856, device='cuda:0')\n",
      "base.4.0.bn1.bias tensor(52, device='cuda:0')\n",
      "base.4.0.bn1.weight tensor(52, device='cuda:0')\n",
      "base.4.0.conv2.weight tensor(2120, device='cuda:0')\n",
      "base.4.0.bn2.bias tensor(53, device='cuda:0')\n",
      "base.4.0.bn2.weight tensor(53, device='cuda:0')\n",
      "base.4.1.conv1.weight tensor(1952, device='cuda:0')\n",
      "base.4.1.bn1.bias tensor(52, device='cuda:0')\n",
      "base.4.1.bn1.weight tensor(52, device='cuda:0')\n",
      "base.4.1.conv2.weight tensor(2319, device='cuda:0')\n",
      "base.4.1.bn2.bias tensor(53, device='cuda:0')\n",
      "base.4.1.bn2.weight tensor(52, device='cuda:0')\n",
      "base.5.0.conv1.weight tensor(1304, device='cuda:0')\n",
      "base.5.0.bn1.bias tensor(104, device='cuda:0')\n",
      "base.5.0.bn1.weight tensor(104, device='cuda:0')\n",
      "base.5.0.conv2.weight tensor(1404, device='cuda:0')\n",
      "base.5.0.bn2.bias tensor(107, device='cuda:0')\n",
      "base.5.0.bn2.weight tensor(105, device='cuda:0')\n",
      "base.5.0.downsample.0.weight tensor(1320, device='cuda:0')\n",
      "base.5.0.downsample.1.bias tensor(107, device='cuda:0')\n",
      "base.5.0.downsample.1.weight tensor(104, device='cuda:0')\n",
      "base.5.1.conv1.weight tensor(1251, device='cuda:0')\n",
      "base.5.1.bn1.bias tensor(104, device='cuda:0')\n",
      "base.5.1.bn1.weight tensor(104, device='cuda:0')\n",
      "base.5.1.conv2.weight tensor(1570, device='cuda:0')\n",
      "base.5.1.bn2.bias tensor(104, device='cuda:0')\n",
      "base.5.1.bn2.weight tensor(104, device='cuda:0')\n",
      "base.6.0.conv1.weight tensor(2643, device='cuda:0')\n",
      "base.6.0.bn1.bias tensor(206, device='cuda:0')\n",
      "base.6.0.bn1.weight tensor(207, device='cuda:0')\n",
      "base.6.0.conv2.weight tensor(2370, device='cuda:0')\n",
      "base.6.0.bn2.bias tensor(212, device='cuda:0')\n",
      "base.6.0.bn2.weight tensor(207, device='cuda:0')\n",
      "base.6.0.downsample.0.weight tensor(1968, device='cuda:0')\n",
      "base.6.0.downsample.1.bias tensor(212, device='cuda:0')\n",
      "base.6.0.downsample.1.weight tensor(208, device='cuda:0')\n",
      "base.6.1.conv1.weight tensor(1479, device='cuda:0')\n",
      "base.6.1.bn1.bias tensor(207, device='cuda:0')\n",
      "base.6.1.bn1.weight tensor(207, device='cuda:0')\n",
      "base.6.1.conv2.weight tensor(1173, device='cuda:0')\n",
      "base.6.1.bn2.bias tensor(208, device='cuda:0')\n",
      "base.6.1.bn2.weight tensor(207, device='cuda:0')\n",
      "base.7.0.conv1.weight tensor(1999, device='cuda:0')\n",
      "base.7.0.bn1.bias tensor(425, device='cuda:0')\n",
      "base.7.0.bn1.weight tensor(415, device='cuda:0')\n",
      "base.7.0.conv2.weight tensor(1319, device='cuda:0')\n",
      "base.7.0.bn2.bias tensor(468, device='cuda:0')\n",
      "base.7.0.bn2.weight tensor(415, device='cuda:0')\n",
      "base.7.0.downsample.0.weight tensor(1491, device='cuda:0')\n",
      "base.7.0.downsample.1.bias tensor(468, device='cuda:0')\n",
      "base.7.0.downsample.1.weight tensor(417, device='cuda:0')\n",
      "base.7.1.conv1.weight tensor(871, device='cuda:0')\n",
      "base.7.1.bn1.bias tensor(422, device='cuda:0')\n",
      "base.7.1.bn1.weight tensor(415, device='cuda:0')\n",
      "base.7.1.conv2.weight tensor(766, device='cuda:0')\n",
      "base.7.1.bn2.bias tensor(451, device='cuda:0')\n",
      "base.7.1.bn2.weight tensor(415, device='cuda:0')\n",
      "final.bias tensor(10, device='cuda:0')\n",
      "final.weight tensor(3418, device='cuda:0')\n",
      "Total Parameters: tensor(44014, device='cuda:0') \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Total number of ternary weights (+w, -w)\n",
    "totalParams = 0\n",
    "for i in layer_name:\n",
    "  print(i,(model.state_dict()[i] !=0).sum())\n",
    "  totalParams +=  (model.state_dict()[i] !=0).sum()\n",
    "    \n",
    "print(\"Total Parameters:\",totalParams, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27472,
     "status": "ok",
     "timestamp": 1656507732213,
     "user": {
      "displayName": "Abhishek kumar Mishra",
      "userId": "13515131134791977528"
     },
     "user_tz": 240
    },
    "id": "0uPbBVUKlJ0X",
    "outputId": "435b409d-2ae8-4a1c-caea-4f8eb37e68ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number Of Images Tested = 10000\n",
      "\n",
      "Model Test Accuracy = 0.5374\n"
     ]
    }
   ],
   "source": [
    "# Model's performance on test set\n",
    "\n",
    "correct_count, all_count = 0, 0\n",
    "model.eval()\n",
    "for images,labels in test_iterator:\n",
    "      for image,label in zip(images,labels):\n",
    "        if torch.cuda.is_available():\n",
    "            img = image.cuda()\n",
    "            lab = label.cuda()\n",
    "            img = img[None,].type('torch.cuda.FloatTensor')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_ = model(img) \n",
    "\n",
    "        pred_label = output_.argmax()\n",
    "\n",
    "        if(pred_label.item()==lab.item()):\n",
    "          correct_count += 1\n",
    "        all_count += 1\n",
    "\n",
    "print(\"Number Of Images Tested =\", all_count)\n",
    "print(\"\\nModel Test Accuracy =\", (correct_count/(all_count)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 462,
     "status": "ok",
     "timestamp": 1656507732666,
     "user": {
      "displayName": "Abhishek kumar Mishra",
      "userId": "13515131134791977528"
     },
     "user_tz": 240
    },
    "id": "RvZCWE-QlJ6U",
    "outputId": "eb98780d-3632-4207-d2a8-8c8bab3f4db7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base.0.weight hidden layer dimension torch.Size([64, 3, 7, 7])\n",
      "Unique values of weight in base.0.weight th hidden layer :  tensor([-0.9980,  0.0000,  0.9980], device='cuda:0')\n",
      "\n",
      "base.1.bias hidden layer dimension torch.Size([64])\n",
      "Unique values of weight in base.1.bias th hidden layer :  tensor([-1.0023,  0.0000,  1.0049], device='cuda:0')\n",
      "\n",
      "base.1.weight hidden layer dimension torch.Size([64])\n",
      "Unique values of weight in base.1.weight th hidden layer :  tensor([0.0000, 1.0063], device='cuda:0')\n",
      "\n",
      "base.4.0.conv1.weight hidden layer dimension torch.Size([64, 64, 3, 3])\n",
      "Unique values of weight in base.4.0.conv1.weight th hidden layer :  tensor([-0.9993,  0.0000,  0.9993], device='cuda:0')\n",
      "\n",
      "base.4.0.bn1.bias hidden layer dimension torch.Size([64])\n",
      "Unique values of weight in base.4.0.bn1.bias th hidden layer :  tensor([-1.0043,  0.0000,  1.0005], device='cuda:0')\n",
      "\n",
      "base.4.0.bn1.weight hidden layer dimension torch.Size([64])\n",
      "Unique values of weight in base.4.0.bn1.weight th hidden layer :  tensor([0.0000, 1.0030], device='cuda:0')\n",
      "\n",
      "base.4.0.conv2.weight hidden layer dimension torch.Size([64, 64, 3, 3])\n",
      "Unique values of weight in base.4.0.conv2.weight th hidden layer :  tensor([-0.9980,  0.0000,  0.9980], device='cuda:0')\n",
      "\n",
      "base.4.0.bn2.bias hidden layer dimension torch.Size([64])\n",
      "Unique values of weight in base.4.0.bn2.bias th hidden layer :  tensor([-1.0084,  0.0000,  1.0035], device='cuda:0')\n",
      "\n",
      "base.4.0.bn2.weight hidden layer dimension torch.Size([64])\n",
      "Unique values of weight in base.4.0.bn2.weight th hidden layer :  tensor([0.0000, 0.9990], device='cuda:0')\n",
      "\n",
      "base.4.1.conv1.weight hidden layer dimension torch.Size([64, 64, 3, 3])\n",
      "Unique values of weight in base.4.1.conv1.weight th hidden layer :  tensor([-0.9982,  0.0000,  0.9982], device='cuda:0')\n",
      "\n",
      "base.4.1.bn1.bias hidden layer dimension torch.Size([64])\n",
      "Unique values of weight in base.4.1.bn1.bias th hidden layer :  tensor([-1.0036,  0.0000,  1.0017], device='cuda:0')\n",
      "\n",
      "base.4.1.bn1.weight hidden layer dimension torch.Size([64])\n",
      "Unique values of weight in base.4.1.bn1.weight th hidden layer :  tensor([0.0000, 1.0017], device='cuda:0')\n",
      "\n",
      "base.4.1.conv2.weight hidden layer dimension torch.Size([64, 64, 3, 3])\n",
      "Unique values of weight in base.4.1.conv2.weight th hidden layer :  tensor([-1.0003,  0.0000,  1.0003], device='cuda:0')\n",
      "\n",
      "base.4.1.bn2.bias hidden layer dimension torch.Size([64])\n",
      "Unique values of weight in base.4.1.bn2.bias th hidden layer :  tensor([-1.0087,  0.0000,  0.9983], device='cuda:0')\n",
      "\n",
      "base.4.1.bn2.weight hidden layer dimension torch.Size([64])\n",
      "Unique values of weight in base.4.1.bn2.weight th hidden layer :  tensor([0.0000, 0.9972], device='cuda:0')\n",
      "\n",
      "base.5.0.conv1.weight hidden layer dimension torch.Size([128, 64, 3, 3])\n",
      "Unique values of weight in base.5.0.conv1.weight th hidden layer :  tensor([-1.0036,  0.0000,  1.0036], device='cuda:0')\n",
      "\n",
      "base.5.0.bn1.bias hidden layer dimension torch.Size([128])\n",
      "Unique values of weight in base.5.0.bn1.bias th hidden layer :  tensor([-1.0057,  0.0000,  1.0006], device='cuda:0')\n",
      "\n",
      "base.5.0.bn1.weight hidden layer dimension torch.Size([128])\n",
      "Unique values of weight in base.5.0.bn1.weight th hidden layer :  tensor([0.0000, 1.0038], device='cuda:0')\n",
      "\n",
      "base.5.0.conv2.weight hidden layer dimension torch.Size([128, 128, 3, 3])\n",
      "Unique values of weight in base.5.0.conv2.weight th hidden layer :  tensor([-1.0007,  0.0000,  1.0007], device='cuda:0')\n",
      "\n",
      "base.5.0.bn2.bias hidden layer dimension torch.Size([128])\n",
      "Unique values of weight in base.5.0.bn2.bias th hidden layer :  tensor([-1.0034,  0.0000,  1.0027], device='cuda:0')\n",
      "\n",
      "base.5.0.bn2.weight hidden layer dimension torch.Size([128])\n",
      "Unique values of weight in base.5.0.bn2.weight th hidden layer :  tensor([0.0000, 0.9999], device='cuda:0')\n",
      "\n",
      "base.5.0.downsample.0.weight hidden layer dimension torch.Size([128, 64, 1, 1])\n",
      "Unique values of weight in base.5.0.downsample.0.weight th hidden layer :  tensor([-0.9982,  0.0000,  0.9982], device='cuda:0')\n",
      "\n",
      "base.5.0.downsample.1.bias hidden layer dimension torch.Size([128])\n",
      "Unique values of weight in base.5.0.downsample.1.bias th hidden layer :  tensor([-1.0034,  0.0000,  1.0027], device='cuda:0')\n",
      "\n",
      "base.5.0.downsample.1.weight hidden layer dimension torch.Size([128])\n",
      "Unique values of weight in base.5.0.downsample.1.weight th hidden layer :  tensor([0.0000, 1.0067], device='cuda:0')\n",
      "\n",
      "base.5.1.conv1.weight hidden layer dimension torch.Size([128, 128, 3, 3])\n",
      "Unique values of weight in base.5.1.conv1.weight th hidden layer :  tensor([-1.0008,  0.0000,  1.0008], device='cuda:0')\n",
      "\n",
      "base.5.1.bn1.bias hidden layer dimension torch.Size([128])\n",
      "Unique values of weight in base.5.1.bn1.bias th hidden layer :  tensor([-1.0060,  0.0000], device='cuda:0')\n",
      "\n",
      "base.5.1.bn1.weight hidden layer dimension torch.Size([128])\n",
      "Unique values of weight in base.5.1.bn1.weight th hidden layer :  tensor([0.0000, 1.0060], device='cuda:0')\n",
      "\n",
      "base.5.1.conv2.weight hidden layer dimension torch.Size([128, 128, 3, 3])\n",
      "Unique values of weight in base.5.1.conv2.weight th hidden layer :  tensor([-0.9999,  0.0000,  0.9999], device='cuda:0')\n",
      "\n",
      "base.5.1.bn2.bias hidden layer dimension torch.Size([128])\n",
      "Unique values of weight in base.5.1.bn2.bias th hidden layer :  tensor([-1.0100,  0.0000,  1.0002], device='cuda:0')\n",
      "\n",
      "base.5.1.bn2.weight hidden layer dimension torch.Size([128])\n",
      "Unique values of weight in base.5.1.bn2.weight th hidden layer :  tensor([0.0000, 0.9948], device='cuda:0')\n",
      "\n",
      "base.6.0.conv1.weight hidden layer dimension torch.Size([256, 128, 3, 3])\n",
      "Unique values of weight in base.6.0.conv1.weight th hidden layer :  tensor([-0.9990,  0.0000,  0.9990], device='cuda:0')\n",
      "\n",
      "base.6.0.bn1.bias hidden layer dimension torch.Size([256])\n",
      "Unique values of weight in base.6.0.bn1.bias th hidden layer :  tensor([-1.0127,  0.0000,  0.9936], device='cuda:0')\n",
      "\n",
      "base.6.0.bn1.weight hidden layer dimension torch.Size([256])\n",
      "Unique values of weight in base.6.0.bn1.weight th hidden layer :  tensor([0.0000, 1.0134], device='cuda:0')\n",
      "\n",
      "base.6.0.conv2.weight hidden layer dimension torch.Size([256, 256, 3, 3])\n",
      "Unique values of weight in base.6.0.conv2.weight th hidden layer :  tensor([-1.0042,  0.0000,  1.0041], device='cuda:0')\n",
      "\n",
      "base.6.0.bn2.bias hidden layer dimension torch.Size([256])\n",
      "Unique values of weight in base.6.0.bn2.bias th hidden layer :  tensor([-1.0014,  0.0000,  1.0028], device='cuda:0')\n",
      "\n",
      "base.6.0.bn2.weight hidden layer dimension torch.Size([256])\n",
      "Unique values of weight in base.6.0.bn2.weight th hidden layer :  tensor([0.0000, 1.0063], device='cuda:0')\n",
      "\n",
      "base.6.0.downsample.0.weight hidden layer dimension torch.Size([256, 128, 1, 1])\n",
      "Unique values of weight in base.6.0.downsample.0.weight th hidden layer :  tensor([-0.9986,  0.0000,  0.9986], device='cuda:0')\n",
      "\n",
      "base.6.0.downsample.1.bias hidden layer dimension torch.Size([256])\n",
      "Unique values of weight in base.6.0.downsample.1.bias th hidden layer :  tensor([-1.0014,  0.0000,  1.0028], device='cuda:0')\n",
      "\n",
      "base.6.0.downsample.1.weight hidden layer dimension torch.Size([256])\n",
      "Unique values of weight in base.6.0.downsample.1.weight th hidden layer :  tensor([-1.0047,  0.0000,  0.9975], device='cuda:0')\n",
      "\n",
      "base.6.1.conv1.weight hidden layer dimension torch.Size([256, 256, 3, 3])\n",
      "Unique values of weight in base.6.1.conv1.weight th hidden layer :  tensor([-0.9996,  0.0000,  0.9996], device='cuda:0')\n",
      "\n",
      "base.6.1.bn1.bias hidden layer dimension torch.Size([256])\n",
      "Unique values of weight in base.6.1.bn1.bias th hidden layer :  tensor([-1.0039,  0.0000], device='cuda:0')\n",
      "\n",
      "base.6.1.bn1.weight hidden layer dimension torch.Size([256])\n",
      "Unique values of weight in base.6.1.bn1.weight th hidden layer :  tensor([0.0000, 1.0039], device='cuda:0')\n",
      "\n",
      "base.6.1.conv2.weight hidden layer dimension torch.Size([256, 256, 3, 3])\n",
      "Unique values of weight in base.6.1.conv2.weight th hidden layer :  tensor([-1.0011,  0.0000,  1.0010], device='cuda:0')\n",
      "\n",
      "base.6.1.bn2.bias hidden layer dimension torch.Size([256])\n",
      "Unique values of weight in base.6.1.bn2.bias th hidden layer :  tensor([-1.0052,  0.0000,  1.0001], device='cuda:0')\n",
      "\n",
      "base.6.1.bn2.weight hidden layer dimension torch.Size([256])\n",
      "Unique values of weight in base.6.1.bn2.weight th hidden layer :  tensor([0.0000, 0.9972], device='cuda:0')\n",
      "\n",
      "base.7.0.conv1.weight hidden layer dimension torch.Size([512, 256, 3, 3])\n",
      "Unique values of weight in base.7.0.conv1.weight th hidden layer :  tensor([-0.9972,  0.0000,  0.9971], device='cuda:0')\n",
      "\n",
      "base.7.0.bn1.bias hidden layer dimension torch.Size([512])\n",
      "Unique values of weight in base.7.0.bn1.bias th hidden layer :  tensor([-1.0133,  0.0000,  1.0029], device='cuda:0')\n",
      "\n",
      "base.7.0.bn1.weight hidden layer dimension torch.Size([512])\n",
      "Unique values of weight in base.7.0.bn1.weight th hidden layer :  tensor([0.0000, 1.0119], device='cuda:0')\n",
      "\n",
      "base.7.0.conv2.weight hidden layer dimension torch.Size([512, 512, 3, 3])\n",
      "Unique values of weight in base.7.0.conv2.weight th hidden layer :  tensor([-0.9794,  0.0000,  0.9794], device='cuda:0')\n",
      "\n",
      "base.7.0.bn2.bias hidden layer dimension torch.Size([512])\n",
      "Unique values of weight in base.7.0.bn2.bias th hidden layer :  tensor([-0.9647,  0.0000,  0.9783], device='cuda:0')\n",
      "\n",
      "base.7.0.bn2.weight hidden layer dimension torch.Size([512])\n",
      "Unique values of weight in base.7.0.bn2.weight th hidden layer :  tensor([0.0000, 0.9599], device='cuda:0')\n",
      "\n",
      "base.7.0.downsample.0.weight hidden layer dimension torch.Size([512, 256, 1, 1])\n",
      "Unique values of weight in base.7.0.downsample.0.weight th hidden layer :  tensor([-0.9850,  0.0000,  0.9849], device='cuda:0')\n",
      "\n",
      "base.7.0.downsample.1.bias hidden layer dimension torch.Size([512])\n",
      "Unique values of weight in base.7.0.downsample.1.bias th hidden layer :  tensor([-0.9647,  0.0000,  0.9783], device='cuda:0')\n",
      "\n",
      "base.7.0.downsample.1.weight hidden layer dimension torch.Size([512])\n",
      "Unique values of weight in base.7.0.downsample.1.weight th hidden layer :  tensor([-1.0000,  0.0000,  0.9638], device='cuda:0')\n",
      "\n",
      "base.7.1.conv1.weight hidden layer dimension torch.Size([512, 512, 3, 3])\n",
      "Unique values of weight in base.7.1.conv1.weight th hidden layer :  tensor([-1.0019,  0.0000,  1.0019], device='cuda:0')\n",
      "\n",
      "base.7.1.bn1.bias hidden layer dimension torch.Size([512])\n",
      "Unique values of weight in base.7.1.bn1.bias th hidden layer :  tensor([-1.0207,  0.0000,  0.9879], device='cuda:0')\n",
      "\n",
      "base.7.1.bn1.weight hidden layer dimension torch.Size([512])\n",
      "Unique values of weight in base.7.1.bn1.weight th hidden layer :  tensor([0.0000, 1.0216], device='cuda:0')\n",
      "\n",
      "base.7.1.conv2.weight hidden layer dimension torch.Size([512, 512, 3, 3])\n",
      "Unique values of weight in base.7.1.conv2.weight th hidden layer :  tensor([-1.0151,  0.0000,  1.0143], device='cuda:0')\n",
      "\n",
      "base.7.1.bn2.bias hidden layer dimension torch.Size([512])\n",
      "Unique values of weight in base.7.1.bn2.bias th hidden layer :  tensor([-0.9805,  0.0000,  0.9251], device='cuda:0')\n",
      "\n",
      "base.7.1.bn2.weight hidden layer dimension torch.Size([512])\n",
      "Unique values of weight in base.7.1.bn2.weight th hidden layer :  tensor([0.0000, 0.9946], device='cuda:0')\n",
      "\n",
      "final.bias hidden layer dimension torch.Size([10])\n",
      "Unique values of weight in final.bias th hidden layer :  tensor([-1.0001,  1.0003], device='cuda:0')\n",
      "\n",
      "final.weight hidden layer dimension torch.Size([10, 512])\n",
      "Unique values of weight in final.weight th hidden layer :  tensor([-1.0668,  0.0000,  0.9294], device='cuda:0')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For each layer, model's ternary weights\n",
    "state_dict = model.state_dict()\n",
    "\n",
    "layer_distinct_weights = {}\n",
    "\n",
    "for i in layer_name:\n",
    "  imd = torch.unique(model.state_dict()[i])\n",
    "  print(i+ ' hidden layer dimension', model.state_dict()[i].shape)\n",
    "  print(\"Unique values of weight in \"+ i+ \" th hidden layer : \", imd)\n",
    "  layer_distinct_weights[i] = imd.cpu().numpy().tolist()\n",
    "  print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1656507732668,
     "user": {
      "displayName": "Abhishek kumar Mishra",
      "userId": "13515131134791977528"
     },
     "user_tz": 240
    },
    "id": "brS36Az4gOPe"
   },
   "outputs": [],
   "source": [
    "img_0 = []; var_0 = 0\n",
    "img_1 = []; var_1 = 0\n",
    "img_2 = []; var_2 = 0\n",
    "img_3 = []; var_3 = 0\n",
    "img_4 = []; var_4 = 0\n",
    "img_5 = []; var_5 = 0\n",
    "img_6 = []; var_6 = 0\n",
    "img_7 = []; var_7 = 0\n",
    "img_8 = []; var_8 = 0\n",
    "img_9 = []; var_9 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 121544,
     "status": "ok",
     "timestamp": 1656507854200,
     "user": {
      "displayName": "Abhishek kumar Mishra",
      "userId": "13515131134791977528"
     },
     "user_tz": 240
    },
    "id": "YISVfn0RgOSB",
    "outputId": "aa1566c0-b380-4ec1-9cfd-4ab719096635"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number Of Images Tested = 40000\n",
      "\n",
      "Model Train Accuracy = 0.538525\n"
     ]
    }
   ],
   "source": [
    "correct_count, all_count = 0, 0\n",
    "np.random.seed(seed_num)\n",
    "\n",
    "num = 5000\n",
    "\n",
    "model.eval()\n",
    "for images,labels in train_iterator:\n",
    "      for image,label in zip(images,labels):\n",
    "\n",
    "        if label == 0:\n",
    "          if var_0 == num:\n",
    "            continue\n",
    "          img_0.append(image.numpy())\n",
    "          var_0 +=1\n",
    "\n",
    "        elif label == 1:\n",
    "          if var_1 == num:\n",
    "            continue\n",
    "          img_1.append(image.numpy())\n",
    "          var_1 +=1\n",
    "\n",
    "        elif label == 2:\n",
    "          if var_2 == num:\n",
    "            continue\n",
    "          img_2.append(image.numpy())\n",
    "          var_2 +=1\n",
    "\n",
    "        elif label == 3:\n",
    "          if var_3 == num:\n",
    "            continue\n",
    "          img_3.append(image.numpy())\n",
    "          var_3 +=1\n",
    "\n",
    "        elif label == 4:\n",
    "          if var_4 == num:\n",
    "            continue\n",
    "          img_4.append(image.numpy())\n",
    "          var_4 +=1\n",
    "\n",
    "        elif label == 5:\n",
    "          if var_5 == num:\n",
    "            continue\n",
    "          img_5.append(image.numpy())\n",
    "          var_5 +=1\n",
    "\n",
    "        elif label == 6:\n",
    "          if var_6 == num:\n",
    "            continue\n",
    "          img_6.append(image.numpy())\n",
    "          var_6 +=1\n",
    "\n",
    "        elif label == 7:\n",
    "          if var_7 == num:\n",
    "            continue\n",
    "          img_7.append(image.numpy())\n",
    "          var_7 +=1\n",
    "\n",
    "        elif label == 8:\n",
    "          if var_8 == num:\n",
    "            continue\n",
    "          img_8.append(image.numpy())\n",
    "          var_8 +=1\n",
    "\n",
    "        elif label == 9:\n",
    "          if var_9 == num:\n",
    "            continue\n",
    "          img_9.append(image.numpy())\n",
    "          var_9 +=1\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            img = image.cuda()\n",
    "            lab = label.cuda()\n",
    "            img = img[None,].type('torch.cuda.FloatTensor')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_ = model(img) \n",
    "\n",
    "        pred_label = output_.argmax()\n",
    "\n",
    "        if(pred_label.item()==lab.item()):\n",
    "          correct_count += 1\n",
    "        all_count += 1\n",
    "\n",
    "print(\"Number Of Images Tested =\", all_count)\n",
    "print(\"\\nModel Train Accuracy =\", (correct_count/(all_count)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1656507854203,
     "user": {
      "displayName": "Abhishek kumar Mishra",
      "userId": "13515131134791977528"
     },
     "user_tz": 240
    },
    "id": "xG1SzTn8gOXE"
   },
   "outputs": [],
   "source": [
    "def getTemplates(imgs, cls):\n",
    "  np.random.seed(seed_num)\n",
    "  corr_images2_1 = []\n",
    "  for img in imgs:\n",
    "    for angle in range(0,360,2):\n",
    "        transform_st = transforms.Compose([transforms.ToTensor(), transforms.RandomRotation(angle), \\\n",
    "                                              transforms.RandomHorizontalFlip(),]) \n",
    "        corr_images2_1.append(transform_st(img).permute(1,2,0).numpy())\n",
    "      \n",
    "        transform_st1 = transforms.Compose([transforms.ToTensor(), transforms.RandomRotation(angle),]) \n",
    "        corr_images2_1.append(transform_st1(img).permute(1,2,0).numpy())\n",
    "      \n",
    "        transform_st2 = transforms.Compose([transforms.ToTensor(),transforms.RandomHorizontalFlip(0.25), ]) \n",
    "        corr_images2_1.append(transform_st2(img).permute(1,2,0).numpy())\n",
    "\n",
    "        transform_st3 = transforms.Compose([transforms.ToTensor(),transforms.RandomHorizontalFlip(0.65), transforms.RandomAffine(angle)]) \n",
    "        corr_images2_1.append(transform_st3(img).permute(1,2,0).numpy())\n",
    "\n",
    "        transform_st4 = transforms.Compose([transforms.ToTensor(),transforms.RandomRotation(angle), transforms.RandomAffine(angle)]) \n",
    "        corr_images2_1.append(transform_st4(img).permute(1,2,0).numpy())\n",
    "\n",
    "        transform_st5 = transforms.Compose([transforms.ToTensor(),transforms.RandomRotation(angle), transforms.RandomHorizontalFlip(0.75),\n",
    "                                            transforms.RandomAffine(angle)]) \n",
    "        corr_images2_1.append(transform_st5(img).permute(1,2,0).numpy())\n",
    "\n",
    "        transform_st6 = transforms.Compose([transforms.ToTensor(),transforms.RandomRotation(angle), transforms.RandomVerticalFlip(0.75),\n",
    "                                            transforms.RandomAffine(angle)]) \n",
    "        corr_images2_1.append(transform_st6(img).permute(1,2,0).numpy())\n",
    "\n",
    "        transform_st7 = transforms.Compose([transforms.ToTensor(), transforms.RandomVerticalFlip(0.75),\n",
    "                                            transforms.RandomAffine(angle)]) \n",
    "        corr_images2_1.append(transform_st7(img).permute(1,2,0).numpy())\n",
    "\n",
    "        transform_st8 = transforms.Compose([transforms.ToTensor(), transforms.RandomVerticalFlip(0.75),]) \n",
    "        corr_images2_1.append(transform_st8(img).permute(1,2,0).numpy())\n",
    "\n",
    "        transform_st9 = transforms.Compose([transforms.ToTensor(), transforms.RandomRotation(angle), \\\n",
    "                                              transforms.RandomVerticalFlip(0.25),]) \n",
    "        corr_images2_1.append(transform_st9(img).permute(1,2,0).numpy())\n",
    "\n",
    "        transform_st10 = transforms.Compose([transforms.ToTensor(), transforms.RandomHorizontalFlip(0.75), transforms.RandomRotation(angle), \\\n",
    "                                              transforms.RandomVerticalFlip(0.75),]) \n",
    "        corr_images2_1.append(transform_st10(img).permute(1,2,0).numpy())\n",
    "\n",
    "        \n",
    "\n",
    "  corr_images2_1 = np.array(corr_images2_1)[np.random.permutation(len(corr_images2_1))]  \n",
    "\n",
    "  corr_images2_1 = torch.tensor(corr_images2_1)\n",
    "\n",
    "  # corr_label = []\n",
    "\n",
    "  # model.eval()\n",
    "  # for img in corr_images2_1:\n",
    "  #   img = img[None,].type('torch.cuda.FloatTensor')\n",
    "  #   with torch.no_grad():\n",
    "  #     pred = model(img).argmax()\n",
    "  #   corr_label.append(pred.item())\n",
    "\n",
    "  # idx = np.where(np.array(corr_label) == cls)\n",
    "\n",
    "  # return np.unique(corr_images2_1[idx], axis=0)[:1000]\n",
    "  \n",
    "  uq_imgs = np.unique(corr_images2_1, axis=0)\n",
    "  return uq_imgs[np.random.permutation(len(uq_imgs))[:1000]]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 42251,
     "status": "ok",
     "timestamp": 1656507896439,
     "user": {
      "displayName": "Abhishek kumar Mishra",
      "userId": "13515131134791977528"
     },
     "user_tz": 240
    },
    "id": "C9zVFUZngOZ-"
   },
   "outputs": [],
   "source": [
    "corr_images_ = []\n",
    "for cls, im in enumerate([img_0, img_1, img_2, img_3, img_4, img_5, img_6, img_7, img_8, img_9]):\n",
    "  corr_images_.append(getTemplates(np.array(im)[np.random.permutation(len(im))[:2]], cls))        # 2 templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1656507896442,
     "user": {
      "displayName": "Abhishek kumar Mishra",
      "userId": "13515131134791977528"
     },
     "user_tz": 240
    },
    "id": "gDc63tH8h0W_",
    "outputId": "d7be53b3-db9e-472a-8bf1-e6e5802c8e97"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3, 32, 32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_images = np.vstack(corr_images_)\n",
    "corr_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20722,
     "status": "ok",
     "timestamp": 1656507917134,
     "user": {
      "displayName": "Abhishek kumar Mishra",
      "userId": "13515131134791977528"
     },
     "user_tz": 240
    },
    "id": "3zyNkvo5o2WB",
    "outputId": "30d7aa29-e98b-461c-8fcd-0623b3d71a8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique digits :  [0 1 2 3 4 5 6 7 8 9]\n",
      "\n",
      " counts :  [7078  275 1495   73   98  104  209  100  211  357]\n"
     ]
    }
   ],
   "source": [
    "corr_images = torch.tensor(corr_images).type(torch.FloatTensor).to('cuda')\n",
    "\n",
    "corr_label = []\n",
    "check_label = []\n",
    "\n",
    "model.eval()\n",
    "for img in corr_images:\n",
    "  img = img[None,].type('torch.cuda.FloatTensor')\n",
    "  with torch.no_grad():\n",
    "    pred = model(img).argmax()\n",
    "  corr_label.append(pred)\n",
    "  check_label.append(pred.item())\n",
    "\n",
    "digit,count = np.unique(check_label, return_counts=True)\n",
    "\n",
    "# Model's prediction on pseudorandom images\n",
    "print(\"\\nUnique digits : \",digit)\n",
    "\n",
    "print(\"\\n counts : \",count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1656507917137,
     "user": {
      "displayName": "Abhishek kumar Mishra",
      "userId": "13515131134791977528"
     },
     "user_tz": 240
    },
    "id": "uG1i7FSwo2Y5"
   },
   "outputs": [],
   "source": [
    "pos_neg_layerwise = {}\n",
    "\n",
    "for i in layer_name:\n",
    "  b = layer_distinct_weights[i]\n",
    "\n",
    "  if len(b) == 3:\n",
    "    neg, zero, pos = b\n",
    "    pos_neg_layerwise[i] = {}\n",
    "    pos_neg_layerwise[i]['pos'] = pos\n",
    "    pos_neg_layerwise[i]['neg'] = neg\n",
    "    \n",
    "  elif len(b) == 2:\n",
    "    un1, un2 = b\n",
    "\n",
    "    if un2 == 0:\n",
    "      neg, zero = b\n",
    "      pos_neg_layerwise[i] = {}\n",
    "      pos_neg_layerwise[i]['neg'] = neg\n",
    "      \n",
    "    elif un1 == 0:\n",
    "      zero, pos = b\n",
    "      pos_neg_layerwise[i] = {}\n",
    "      pos_neg_layerwise[i]['pos'] = pos\n",
    "\n",
    "    else:\n",
    "      neg, pos = b\n",
    "      pos_neg_layerwise[i] = {}\n",
    "      pos_neg_layerwise[i]['pos'] = pos\n",
    "      pos_neg_layerwise[i]['neg'] = neg\n",
    "\n",
    "  else:\n",
    "     un = b[0] \n",
    "\n",
    "     if un > 0:\n",
    "       pos_neg_layerwise[i] = {}\n",
    "       pos_neg_layerwise[i]['pos'] = un\n",
    "\n",
    "     elif un < 0:\n",
    "       pos_neg_layerwise[i] = {}\n",
    "       pos_neg_layerwise[i]['neg'] = un\n",
    "\n",
    "     else:\n",
    "       pos_neg_layerwise[i] = {}\n",
    "       pos_neg_layerwise[i]['zero'] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 32,
     "status": "ok",
     "timestamp": 1656507917139,
     "user": {
      "displayName": "Abhishek kumar Mishra",
      "userId": "13515131134791977528"
     },
     "user_tz": 240
    },
    "id": "JZ4zH-VAo2bH",
    "outputId": "19109f78-2e14-4f67-b2fe-8383e7e541fc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base.0.weight': [-0.997970700263977, 0.0, 0.997970700263977],\n",
       " 'base.1.bias': [-1.002286434173584, 0.0, 1.0049375295639038],\n",
       " 'base.1.weight': [0.0, 1.0062828063964844],\n",
       " 'base.4.0.bn1.bias': [-1.0043219327926636, 0.0, 1.000535011291504],\n",
       " 'base.4.0.bn1.weight': [0.0, 1.0029528141021729],\n",
       " 'base.4.0.bn2.bias': [-1.0083696842193604, 0.0, 1.0035288333892822],\n",
       " 'base.4.0.bn2.weight': [0.0, 0.9989628195762634],\n",
       " 'base.4.0.conv1.weight': [-0.9992982745170593, 0.0, 0.9992982149124146],\n",
       " 'base.4.0.conv2.weight': [-0.9979877471923828, 0.0, 0.9979877471923828],\n",
       " 'base.4.1.bn1.bias': [-1.0035675764083862, 0.0, 1.0016995668411255],\n",
       " 'base.4.1.bn1.weight': [0.0, 1.001746654510498],\n",
       " 'base.4.1.bn2.bias': [-1.008662223815918, 0.0, 0.998275637626648],\n",
       " 'base.4.1.bn2.weight': [0.0, 0.9972245097160339],\n",
       " 'base.4.1.conv1.weight': [-0.9981896281242371, 0.0, 0.9981896281242371],\n",
       " 'base.4.1.conv2.weight': [-1.0003037452697754, 0.0, 1.0003037452697754],\n",
       " 'base.5.0.bn1.bias': [-1.0056942701339722, 0.0, 1.0005754232406616],\n",
       " 'base.5.0.bn1.weight': [0.0, 1.0037789344787598],\n",
       " 'base.5.0.bn2.bias': [-1.0034257173538208, 0.0, 1.002722144126892],\n",
       " 'base.5.0.bn2.weight': [0.0, 0.9999319314956665],\n",
       " 'base.5.0.conv1.weight': [-1.0036280155181885, 0.0, 1.0036280155181885],\n",
       " 'base.5.0.conv2.weight': [-1.0006707906723022, 0.0, 1.0006706714630127],\n",
       " 'base.5.0.downsample.0.weight': [-0.9982104897499084,\n",
       "  0.0,\n",
       "  0.9982104897499084],\n",
       " 'base.5.0.downsample.1.bias': [-1.0034257173538208, 0.0, 1.002722144126892],\n",
       " 'base.5.0.downsample.1.weight': [0.0, 1.0066616535186768],\n",
       " 'base.5.1.bn1.bias': [-1.0059788227081299, 0.0],\n",
       " 'base.5.1.bn1.weight': [0.0, 1.0059787034988403],\n",
       " 'base.5.1.bn2.bias': [-1.0099973678588867, 0.0, 1.0002497434616089],\n",
       " 'base.5.1.bn2.weight': [0.0, 0.9948470592498779],\n",
       " 'base.5.1.conv1.weight': [-1.000842809677124, 0.0, 1.0008429288864136],\n",
       " 'base.5.1.conv2.weight': [-0.9999493360519409, 0.0, 0.9999484419822693],\n",
       " 'base.6.0.bn1.bias': [-1.0127111673355103, 0.0, 0.993628203868866],\n",
       " 'base.6.0.bn1.weight': [0.0, 1.013394832611084],\n",
       " 'base.6.0.bn2.bias': [-1.0013892650604248, 0.0, 1.002809762954712],\n",
       " 'base.6.0.bn2.weight': [0.0, 1.0063414573669434],\n",
       " 'base.6.0.conv1.weight': [-0.9990434050559998, 0.0, 0.9990435838699341],\n",
       " 'base.6.0.conv2.weight': [-1.0041515827178955, 0.0, 1.0041072368621826],\n",
       " 'base.6.0.downsample.0.weight': [-0.9986178874969482,\n",
       "  0.0,\n",
       "  0.9986178874969482],\n",
       " 'base.6.0.downsample.1.bias': [-1.0013892650604248, 0.0, 1.002809762954712],\n",
       " 'base.6.0.downsample.1.weight': [-1.0046577453613281,\n",
       "  0.0,\n",
       "  0.9974761605262756],\n",
       " 'base.6.1.bn1.bias': [-1.0039376020431519, 0.0],\n",
       " 'base.6.1.bn1.weight': [0.0, 1.0038793087005615],\n",
       " 'base.6.1.bn2.bias': [-1.0051746368408203, 0.0, 1.0001323223114014],\n",
       " 'base.6.1.bn2.weight': [0.0, 0.997181236743927],\n",
       " 'base.6.1.conv1.weight': [-0.9996164441108704, 0.0, 0.9996131658554077],\n",
       " 'base.6.1.conv2.weight': [-1.0010912418365479, 0.0, 1.0010483264923096],\n",
       " 'base.7.0.bn1.bias': [-1.0132701396942139, 0.0, 1.0029336214065552],\n",
       " 'base.7.0.bn1.weight': [0.0, 1.0118685960769653],\n",
       " 'base.7.0.bn2.bias': [-0.9646565914154053, 0.0, 0.9782690405845642],\n",
       " 'base.7.0.bn2.weight': [0.0, 0.959900438785553],\n",
       " 'base.7.0.conv1.weight': [-0.9971515536308289, 0.0, 0.9971283078193665],\n",
       " 'base.7.0.conv2.weight': [-0.9794412851333618, 0.0, 0.9794382452964783],\n",
       " 'base.7.0.downsample.0.weight': [-0.9849801063537598,\n",
       "  0.0,\n",
       "  0.9849342107772827],\n",
       " 'base.7.0.downsample.1.bias': [-0.9646565914154053, 0.0, 0.9782690405845642],\n",
       " 'base.7.0.downsample.1.weight': [-0.9999861717224121,\n",
       "  0.0,\n",
       "  0.9638087749481201],\n",
       " 'base.7.1.bn1.bias': [-1.0207003355026245, 0.0, 0.9878734350204468],\n",
       " 'base.7.1.bn1.weight': [0.0, 1.021633267402649],\n",
       " 'base.7.1.bn2.bias': [-0.9805426001548767, 0.0, 0.9250654578208923],\n",
       " 'base.7.1.bn2.weight': [0.0, 0.9945676326751709],\n",
       " 'base.7.1.conv1.weight': [-1.0018998384475708, 0.0, 1.0018991231918335],\n",
       " 'base.7.1.conv2.weight': [-1.0150518417358398, 0.0, 1.014282464981079],\n",
       " 'final.bias': [-1.0000823736190796, 1.000348687171936],\n",
       " 'final.weight': [-1.0667845010757446, 0.0, 0.929437518119812]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_distinct_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1656507917141,
     "user": {
      "displayName": "Abhishek kumar Mishra",
      "userId": "13515131134791977528"
     },
     "user_tz": 240
    },
    "id": "cCfKOm8no2du"
   },
   "outputs": [],
   "source": [
    "class PseudoData(Dataset):\n",
    "    \n",
    "    def __init__(self, data, label):\n",
    "        \n",
    "        self.data = data\n",
    "        self.label = label\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        return self.data[index], self.label[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "corr_label = torch.tensor(corr_label).type(torch.FloatTensor).to('cuda')\n",
    "pseudo_dataLoader = DataLoader(dataset = PseudoData(corr_images, corr_label), batch_size = 1000, shuffle=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1656507917142,
     "user": {
      "displayName": "Abhishek kumar Mishra",
      "userId": "13515131134791977528"
     },
     "user_tz": 240
    },
    "id": "uAf424TSo2gc"
   },
   "outputs": [],
   "source": [
    "def getResult(test_example, test_label, model, st):\n",
    "    model.load_state_dict(st)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output_ = model(test_example.float()) \n",
    "        pred = output_.data.max(1, keepdim=True)[1]\n",
    "        z = pred.eq(test_label.data.view_as(pred)).flatten()\n",
    "        return torch.where(z == False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1656507917439,
     "user": {
      "displayName": "Abhishek kumar Mishra",
      "userId": "13515131134791977528"
     },
     "user_tz": 240
    },
    "id": "qQkUUItGo2j2"
   },
   "outputs": [],
   "source": [
    "def populateResults(idx, batch_id, old_state, new_state, w_images, n_ic, n_oc, row, column, weight_name):\n",
    "  key = 'img_id'+str(idx)+'_batch_'+str(batch_id)\n",
    "  if key not in w_images:\n",
    "    w_images[key] = {}\n",
    "    w_images[key]['location'] = []\n",
    "    w_images[key]['location'].append((weight_name, n_ic, n_oc, row, column, old_state, new_state))\n",
    "    w_images[key]['weight_states'] = []\n",
    "    w_images[key]['weight_states'].append(weight_name + ' : ' + str(old_state) + ' --> ' + str(new_state))\n",
    "\n",
    "  else:\n",
    "    w_images[key]['location'].append((weight_name, n_ic, n_oc, row, column, old_state, new_state))\n",
    "    w_images[key]['weight_states'].append(weight_name + ' : ' + str(old_state) + ' --> ' + str(new_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1656507917440,
     "user": {
      "displayName": "Abhishek kumar Mishra",
      "userId": "13515131134791977528"
     },
     "user_tz": 240
    },
    "id": "jCBF5EBvo2lz"
   },
   "outputs": [],
   "source": [
    "def populateResults1(idx, batch_id, old_state, new_state, w_images, row, column, weight_name):\n",
    "  key = 'img_id'+str(idx)+'_batch_'+str(batch_id)\n",
    "  if key not in w_images:\n",
    "    w_images[key] = {}\n",
    "    w_images[key]['location'] = []\n",
    "    w_images[key]['location'].append((weight_name, row, column, old_state, new_state))\n",
    "    w_images[key]['weight_states'] = []\n",
    "    w_images[key]['weight_states'].append(weight_name + ' : ' + str(old_state) + ' --> ' + str(new_state))\n",
    "\n",
    "  else:\n",
    "    w_images[key]['location'].append((weight_name, row, column, old_state, new_state))\n",
    "    w_images[key]['weight_states'].append(weight_name + ' : ' + str(old_state) + ' --> ' + str(new_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 32,
     "status": "ok",
     "timestamp": 1656507917442,
     "user": {
      "displayName": "Abhishek kumar Mishra",
      "userId": "13515131134791977528"
     },
     "user_tz": 240
    },
    "id": "Tb0jVhzUo2oX"
   },
   "outputs": [],
   "source": [
    "# For bias weight mutation\n",
    "\n",
    "def bias_mutation(st, superSet, w_images, img,label, batch_id, weight_name):\n",
    "\n",
    "  if ('pos' in pos_neg_layerwise[weight_name]) and ('neg' in pos_neg_layerwise[weight_name]):\n",
    "    un = pos_neg_layerwise[weight_name]['pos'], pos_neg_layerwise[weight_name]['neg']\n",
    "\n",
    "    var_neg1 = torch.where(st==un[1])[0]\n",
    "    var_1 = torch.where(st==un[0])[0]\n",
    "\n",
    "  else:\n",
    "    try:\n",
    "      un = pos_neg_layerwise[weight_name]['pos']\n",
    "\n",
    "      var_neg1 = torch.where(st==un*1000)[0]\n",
    "      var_1 = torch.where(st==un)[0]\n",
    "\n",
    "    except KeyError:\n",
    "      un = pos_neg_layerwise[weight_name]['neg']\n",
    "\n",
    "      var_neg1 = torch.where(st==un)[0]\n",
    "      var_1 = torch.where(st==un*1000)[0]\n",
    "\n",
    "\n",
    "  if (var_neg1.nelement() == 0.) and (var_1.nelement() == 0.):\n",
    "    pass\n",
    "  elif (var_1.nelement() > 0):\n",
    "    for column, column1, column2 in zip(var_1, torch.flip(var_1, dims=(0,)), torch.roll(var_1, 2)):\n",
    "      row = 0\n",
    "      column = column.item()\n",
    "      column1 = column1.item()\n",
    "      column2 = column2.item()\n",
    "\n",
    "      org_val_real = st[column].item() # For bias part\n",
    "\n",
    "      if type(un) == tuple:\n",
    "          pos, neg = un   # Positive, Negative \n",
    "\n",
    "          \"\"\"getResult() will tell whether the passed test example is classified to other class\n",
    "          or not after mutation, if it is classified to other class, then it returns True else False and \n",
    "          populateResults() will maintain the record of test examples which has been misclassified\n",
    "          like location(row(which neuron), column(indices)), old_state(old_value of weight element), \n",
    "          new_state (mutated value of weight element), images\"\"\"\n",
    "\n",
    "          if (org_val_real > 0.) and (org_val_real == pos):\n",
    "\n",
    "            org_val = pos\n",
    "            old_state = pos\n",
    "            new_state = 0.\n",
    "            superSet.add((weight_name, row, column, 1, 0))\n",
    "\n",
    "            # Injecting multiple faults (three)\n",
    "            st[column] = new_state\n",
    "            st[column1] = new_state\n",
    "            st[column2] = new_state\n",
    "\n",
    "            [populateResults1(idx.item(), batch_id, 1, 0, w_images, row, column, weight_name) \\\n",
    "             for idx in getResult(img, label, model, state_dict)]\n",
    "          \n",
    "            new_state = neg\n",
    "            superSet.add((weight_name, row, column, 1, -1))\n",
    "\n",
    "            # Injecting multiple faults (three)\n",
    "            st[column] = new_state\n",
    "            st[column1] = new_state\n",
    "            st[column2] = new_state\n",
    "            \n",
    "            [populateResults1(idx.item(), batch_id, 1, -1, w_images, row, column, weight_name) \\\n",
    "             for idx in getResult(img, label, model, state_dict)]\n",
    "            \n",
    "            st[column] = org_val\n",
    "            st[column1] = org_val\n",
    "            st[column2] = org_val\n",
    "\n",
    "\n",
    "\n",
    "      elif (un > 0) and (org_val_real == un):  ############# Positive  ################################\n",
    "          org_val = un\n",
    "          old_state = un\n",
    "          new_state = 0.\n",
    "          superSet.add((weight_name, row, column, 1, 0))\n",
    "\n",
    "          # Injecting multiple faults (two)\n",
    "          st[column] = new_state\n",
    "          st[column1] = new_state\n",
    "          st[column2] = new_state\n",
    "\n",
    "          [populateResults1(idx.item(), batch_id, 1, 0, w_images, row, column, weight_name) \\\n",
    "           for idx in getResult(img, label, model, state_dict)]\n",
    "\n",
    "          st[column] = org_val\n",
    "          st[column1] = org_val\n",
    "          st[column2] = org_val\n",
    "\n",
    "\n",
    "      #################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1656507917442,
     "user": {
      "displayName": "Abhishek kumar Mishra",
      "userId": "13515131134791977528"
     },
     "user_tz": 240
    },
    "id": "oaZX0ddzo2rG"
   },
   "outputs": [],
   "source": [
    "# Saving length of shape for each layer\n",
    "layers_shape = {}\n",
    "\n",
    "for i in layer_name:\n",
    "  layers_shape[i] = len(state_dict[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1656507917443,
     "user": {
      "displayName": "Abhishek kumar Mishra",
      "userId": "13515131134791977528"
     },
     "user_tz": 240
    },
    "id": "PFbz-pOzo2tw",
    "outputId": "45efcbdc-f6dc-451f-a278-deb58722b973"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'base.0.weight': 4, 'base.1.bias': 1, 'base.1.weight': 1, 'base.4.0.conv1.weight': 4, 'base.4.0.bn1.bias': 1, 'base.4.0.bn1.weight': 1, 'base.4.0.conv2.weight': 4, 'base.4.0.bn2.bias': 1, 'base.4.0.bn2.weight': 1, 'base.4.1.conv1.weight': 4, 'base.4.1.bn1.bias': 1, 'base.4.1.bn1.weight': 1, 'base.4.1.conv2.weight': 4, 'base.4.1.bn2.bias': 1, 'base.4.1.bn2.weight': 1, 'base.5.0.conv1.weight': 4, 'base.5.0.bn1.bias': 1, 'base.5.0.bn1.weight': 1, 'base.5.0.conv2.weight': 4, 'base.5.0.bn2.bias': 1, 'base.5.0.bn2.weight': 1, 'base.5.0.downsample.0.weight': 4, 'base.5.0.downsample.1.bias': 1, 'base.5.0.downsample.1.weight': 1, 'base.5.1.conv1.weight': 4, 'base.5.1.bn1.bias': 1, 'base.5.1.bn1.weight': 1, 'base.5.1.conv2.weight': 4, 'base.5.1.bn2.bias': 1, 'base.5.1.bn2.weight': 1, 'base.6.0.conv1.weight': 4, 'base.6.0.bn1.bias': 1, 'base.6.0.bn1.weight': 1, 'base.6.0.conv2.weight': 4, 'base.6.0.bn2.bias': 1, 'base.6.0.bn2.weight': 1, 'base.6.0.downsample.0.weight': 4, 'base.6.0.downsample.1.bias': 1, 'base.6.0.downsample.1.weight': 1, 'base.6.1.conv1.weight': 4, 'base.6.1.bn1.bias': 1, 'base.6.1.bn1.weight': 1, 'base.6.1.conv2.weight': 4, 'base.6.1.bn2.bias': 1, 'base.6.1.bn2.weight': 1, 'base.7.0.conv1.weight': 4, 'base.7.0.bn1.bias': 1, 'base.7.0.bn1.weight': 1, 'base.7.0.conv2.weight': 4, 'base.7.0.bn2.bias': 1, 'base.7.0.bn2.weight': 1, 'base.7.0.downsample.0.weight': 4, 'base.7.0.downsample.1.bias': 1, 'base.7.0.downsample.1.weight': 1, 'base.7.1.conv1.weight': 4, 'base.7.1.bn1.bias': 1, 'base.7.1.bn1.weight': 1, 'base.7.1.conv2.weight': 4, 'base.7.1.bn2.bias': 1, 'base.7.1.bn2.weight': 1, 'final.bias': 1, 'final.weight': 2}\n"
     ]
    }
   ],
   "source": [
    "print(layers_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16337529,
     "status": "ok",
     "timestamp": 1656524254959,
     "user": {
      "displayName": "Abhishek kumar Mishra",
      "userId": "13515131134791977528"
     },
     "user_tz": 240
    },
    "id": "NuNNlCH0o2wG",
    "outputId": "3e4edad4-9db1-4111-fefc-5c1c09c9e853"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of elements in superSet is : 44994. \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|         | 1/10 [26:57<4:02:37, 1617.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 17219 locations that did not match.\n",
      "\n",
      "Total number of locations 27775 are encompassed out of 44994.\n",
      "Total number of elements in a weight matrix is  44994\n",
      "Total number of images fetched  41 \n",
      "\n",
      "Output \n",
      "\n",
      "[(41, 27775, 44994)]\n",
      "Total number of elements in superSet is : 44994. \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|        | 2/10 [54:05<3:36:28, 1623.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8912 locations that did not match.\n",
      "\n",
      "Total number of locations 36082 are encompassed out of 44994.\n",
      "Total number of elements in a weight matrix is  44994\n",
      "Total number of images fetched  99 \n",
      "\n",
      "Output \n",
      "\n",
      "[(41, 27775, 44994), (99, 36082, 44994)]\n",
      "Total number of elements in superSet is : 44994. \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|       | 3/10 [1:21:11<3:09:34, 1624.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8132 locations that did not match.\n",
      "\n",
      "Total number of locations 36862 are encompassed out of 44994.\n",
      "Total number of elements in a weight matrix is  44994\n",
      "Total number of images fetched  114 \n",
      "\n",
      "Output \n",
      "\n",
      "[(41, 27775, 44994), (99, 36082, 44994), (114, 36862, 44994)]\n",
      "Total number of elements in superSet is : 44994. \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|      | 4/10 [1:48:20<2:42:38, 1626.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7417 locations that did not match.\n",
      "\n",
      "Total number of locations 37577 are encompassed out of 44994.\n",
      "Total number of elements in a weight matrix is  44994\n",
      "Total number of images fetched  143 \n",
      "\n",
      "Output \n",
      "\n",
      "[(41, 27775, 44994), (99, 36082, 44994), (114, 36862, 44994), (143, 37577, 44994)]\n",
      "Total number of elements in superSet is : 44994. \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|     | 5/10 [2:15:23<2:15:24, 1624.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7252 locations that did not match.\n",
      "\n",
      "Total number of locations 37742 are encompassed out of 44994.\n",
      "Total number of elements in a weight matrix is  44994\n",
      "Total number of images fetched  145 \n",
      "\n",
      "Output \n",
      "\n",
      "[(41, 27775, 44994), (99, 36082, 44994), (114, 36862, 44994), (143, 37577, 44994), (145, 37742, 44994)]\n",
      "Total number of elements in superSet is : 44994. \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|    | 6/10 [2:42:32<1:48:26, 1626.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6902 locations that did not match.\n",
      "\n",
      "Total number of locations 38092 are encompassed out of 44994.\n",
      "Total number of elements in a weight matrix is  44994\n",
      "Total number of images fetched  156 \n",
      "\n",
      "Output \n",
      "\n",
      "[(41, 27775, 44994), (99, 36082, 44994), (114, 36862, 44994), (143, 37577, 44994), (145, 37742, 44994), (156, 38092, 44994)]\n",
      "Total number of elements in superSet is : 44994. \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|   | 7/10 [3:09:48<1:21:28, 1629.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6656 locations that did not match.\n",
      "\n",
      "Total number of locations 38338 are encompassed out of 44994.\n",
      "Total number of elements in a weight matrix is  44994\n",
      "Total number of images fetched  167 \n",
      "\n",
      "Output \n",
      "\n",
      "[(41, 27775, 44994), (99, 36082, 44994), (114, 36862, 44994), (143, 37577, 44994), (145, 37742, 44994), (156, 38092, 44994), (167, 38338, 44994)]\n",
      "Total number of elements in superSet is : 44994. \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|  | 8/10 [3:37:12<54:28, 1634.38s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6408 locations that did not match.\n",
      "\n",
      "Total number of locations 38586 are encompassed out of 44994.\n",
      "Total number of elements in a weight matrix is  44994\n",
      "Total number of images fetched  182 \n",
      "\n",
      "Output \n",
      "\n",
      "[(41, 27775, 44994), (99, 36082, 44994), (114, 36862, 44994), (143, 37577, 44994), (145, 37742, 44994), (156, 38092, 44994), (167, 38338, 44994), (182, 38586, 44994)]\n",
      "Total number of elements in superSet is : 44994. \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%| | 9/10 [4:04:42<27:19, 1639.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5974 locations that did not match.\n",
      "\n",
      "Total number of locations 39020 are encompassed out of 44994.\n",
      "Total number of elements in a weight matrix is  44994\n",
      "Total number of images fetched  199 \n",
      "\n",
      "Output \n",
      "\n",
      "[(41, 27775, 44994), (99, 36082, 44994), (114, 36862, 44994), (143, 37577, 44994), (145, 37742, 44994), (156, 38092, 44994), (167, 38338, 44994), (182, 38586, 44994), (199, 39020, 44994)]\n",
      "Total number of elements in superSet is : 44994. \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 10/10 [4:32:16<00:00, 1633.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5621 locations that did not match.\n",
      "\n",
      "Total number of locations 39373 are encompassed out of 44994.\n",
      "Total number of elements in a weight matrix is  44994\n",
      "Total number of images fetched  209 \n",
      "\n",
      "Output \n",
      "\n",
      "[(41, 27775, 44994), (99, 36082, 44994), (114, 36862, 44994), (143, 37577, 44994), (145, 37742, 44994), (156, 38092, 44994), (167, 38338, 44994), (182, 38586, 44994), (199, 39020, 44994), (209, 39373, 44994)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Injecting fault by mutating the ternary weight and check whether it is classified to other class than before by the model. \n",
    "\n",
    "fetchimg_totImg_coverage_totCov = []\n",
    "\n",
    "w_images = {}\n",
    "superSet = set()\n",
    "\n",
    "\n",
    "batch_id = 0\n",
    "for img,label  in tqdm(pseudo_dataLoader):\n",
    "\n",
    "    for weight_name in layer_name:\n",
    "\n",
    "        # Selecting hidden layer to iterate its weight elements\n",
    "        st = state_dict[weight_name]\n",
    "\n",
    "\n",
    "        if ('bias' in weight_name) or (layers_shape[weight_name] == 1):\n",
    "          bias_mutation(st, superSet, w_images, img,label, batch_id, weight_name)\n",
    "          continue\n",
    "\n",
    "        if ('conv' in weight_name) or (layers_shape[weight_name] == 4):\n",
    "\n",
    "          ic, oc, k,k1 = st.shape # ic = in_channel, oc= out_channel, k and k1 is the number of rows and columns of a kernel\n",
    "\n",
    "          for n_ic in range(ic):\n",
    "            for n_oc in range(oc):\n",
    "              kt =  st[n_ic][n_oc] # Fetching Kernel matrix or tensor\n",
    "\n",
    "              # Iterating number of neurons for the selected hidden layer\n",
    "              for row in range(k):\n",
    "\n",
    "                # Fetching indices for -w and w weight value for the selected kernel\n",
    "                \"\"\"Since each kernel has weight vector, so by fetching indices\n",
    "                we will come to know, which indices of the kernel contains weights -w or w weight\"\"\"\n",
    "\n",
    "                ###########################################################################################\n",
    "\n",
    "                if ('pos' in pos_neg_layerwise[weight_name]) and ('neg' in pos_neg_layerwise[weight_name]):\n",
    "                  un = pos_neg_layerwise[weight_name]['pos'], pos_neg_layerwise[weight_name]['neg']\n",
    "\n",
    "                  var_neg1 = torch.where(kt[row]==un[1])[0]\n",
    "                  var_1 = torch.where(kt[row]==un[0])[0]\n",
    "\n",
    "                else:\n",
    "                  try:\n",
    "                    un = pos_neg_layerwise[weight_name]['pos']\n",
    "\n",
    "                    var_neg1 = torch.where(kt[row]==un*1000)[0]\n",
    "                    var_1 = torch.where(kt[row]==un)[0]\n",
    "\n",
    "                  except KeyError:\n",
    "                    un = pos_neg_layerwise[weight_name]['neg']\n",
    "\n",
    "                    var_neg1 = torch.where(kt[row]==un)[0]\n",
    "                    var_1 = torch.where(kt[row]==un*1000)[0]\n",
    "\n",
    "                ##########################################################################################\n",
    "  \n",
    "\n",
    "                if (var_neg1.nelement() == 0.) and (var_1.nelement() == 0.):\n",
    "                  continue\n",
    "                elif (var_1.nelement() > 0):\n",
    "                  for column, column1, column2 in zip(var_1, torch.flip(var_1, dims=(0,)), torch.roll(var_1, 2)):\n",
    "                    column = column.item()\n",
    "                    column1 = column1.item()\n",
    "                    column2 = column2.item()\n",
    "\n",
    "                    org_val_real = kt[row][column].item()\n",
    "\n",
    "                    ##################################################################################################################\n",
    "\n",
    "                    if type(un) == tuple:\n",
    "                      pos, neg = un  # Positive, Negative\n",
    "                        \n",
    "                      \"\"\"getResult() will tell whether the passed test example is classified to other class\n",
    "                      or not after mutation, if it is classified to other class, then it returns True else False and \n",
    "                      populateResults() will maintain the record of test examples which has been misclassified\n",
    "                      like location(row(which neuron), column(indices)), old_state(old_value of weight element), \n",
    "                      new_state (mutated value of weight element), images\"\"\"\n",
    "\n",
    "                      if (org_val_real > 0.) and (org_val_real == pos):\n",
    "                        org_val = pos\n",
    "                        old_state = pos\n",
    "                        new_state = 0.\n",
    "                        superSet.add((weight_name, n_ic, n_oc, row, column, 1, 0))\n",
    "\n",
    "                        # Injecting multiple faults (three)\n",
    "                        st[n_ic][n_oc][row][column] = new_state\n",
    "                        st[n_ic][n_oc][row][column1] = new_state\n",
    "                        st[n_ic][n_oc][row][column2] = new_state\n",
    "                        \n",
    "                        [populateResults(idx.item(), batch_id, 1, 0, w_images, n_ic, n_oc, row, column, weight_name) \\\n",
    "                         for idx in getResult(img, label, model, state_dict)]\n",
    "                        \n",
    "                        new_state = neg\n",
    "                        superSet.add((weight_name, n_ic, n_oc, row, column, 1, -1))\n",
    "\n",
    "                        # Injecting multiple faults (three)\n",
    "                        st[n_ic][n_oc][row][column] = new_state\n",
    "                        st[n_ic][n_oc][row][column1] = new_state\n",
    "                        st[n_ic][n_oc][row][column2] = new_state\n",
    "\n",
    "                            \n",
    "                        [populateResults(idx.item(), batch_id, 1, -1, w_images, n_ic, n_oc, row, column, weight_name) \\\n",
    "                         for idx in getResult(img, label, model, state_dict)]\n",
    "                        \n",
    "                        st[n_ic][n_oc][row][column] = org_val # Setting back to original value\n",
    "                        st[n_ic][n_oc][row][column1] = org_val\n",
    "                        st[n_ic][n_oc][row][column2] = org_val\n",
    "\n",
    "\n",
    "\n",
    "                    elif (un > 0) and (org_val_real == un): ########################## Positive ###########################################\n",
    "                      org_val = un\n",
    "                      old_state = un\n",
    "                      new_state = 0.\n",
    "                      superSet.add((weight_name, n_ic, n_oc, row, column, 1, 0))\n",
    "\n",
    "                      # Injecting multiple faults (three)\n",
    "                      st[n_ic][n_oc][row][column] = new_state\n",
    "                      st[n_ic][n_oc][row][column1] = new_state\n",
    "                      st[n_ic][n_oc][row][column2] = new_state\n",
    "                        \n",
    "                      [populateResults(idx.item(), batch_id, 1, 0, w_images, n_ic, n_oc, row, column, weight_name) \\\n",
    "                       for idx in getResult(img, label, model, state_dict)]\n",
    "                      st[n_ic][n_oc][row][column] = org_val\n",
    "                      st[n_ic][n_oc][row][column1] = org_val\n",
    "                      st[n_ic][n_oc][row][column2] = org_val\n",
    "\n",
    "\n",
    "        \n",
    "        # For fully connected layers\n",
    "        elif layers_shape[weight_name] == 2:\n",
    "          rows = state_dict[weight_name].shape[0]\n",
    "\n",
    "          # Iterating number of neurons for the selected hidden layer\n",
    "          for row in range(rows):\n",
    "\n",
    "            # Fetching indices for -w and w weight value for the selected neuron\n",
    "            \"\"\"Since each neuron has weight vector, so by fetching indices\n",
    "            we will come to know, which indices of the neuron contains weights -w or w weight\"\"\"\n",
    "\n",
    "            #########################################################################################\n",
    "\n",
    "            if ('pos' in pos_neg_layerwise[weight_name]) and ('neg' in pos_neg_layerwise[weight_name]):\n",
    "              un = pos_neg_layerwise[weight_name]['pos'], pos_neg_layerwise[weight_name]['neg']\n",
    "\n",
    "              var_neg1 = torch.where(st[row]==un[1])[0]\n",
    "              var_1 = torch.where(st[row]==un[0])[0]\n",
    "\n",
    "            else:\n",
    "              try:\n",
    "                un = pos_neg_layerwise[weight_name]['pos']\n",
    "\n",
    "                var_neg1 = torch.where(st[row]==un*1000)[0]\n",
    "                var_1 = torch.where(st[row]==un)[0]\n",
    "\n",
    "              except KeyError:\n",
    "                un = pos_neg_layerwise[weight_name]['neg']\n",
    "\n",
    "                var_neg1 = torch.where(st[row]==un)[0]\n",
    "                var_1 = torch.where(st[row]==un*1000)[0]\n",
    "\n",
    "\n",
    "            #########################################################################################\n",
    "                  \n",
    "            if (var_neg1.nelement() == 0.) and (var_1.nelement() == 0.):\n",
    "              continue\n",
    "            elif (var_1.nelement() > 0):\n",
    "              for column, column1, column2 in zip(var_1, torch.flip(var_1, dims=(0,)), torch.roll(var_1, 2)):\n",
    "\n",
    "                column = column.item()\n",
    "                column1 = column1.item()\n",
    "                column2 = column2.item()\n",
    "\n",
    "                org_val_v1 = st[row][column].item()\n",
    "\n",
    "\n",
    "                #########################################################################################\n",
    "\n",
    "                if type(un) == tuple:\n",
    "                  pos, neg = un  # Positive, Negative\n",
    "\n",
    "                  if (org_val_v1 > 0.) and (pos == org_val_v1):\n",
    "\n",
    "                    org_val = pos\n",
    "                    old_state = pos\n",
    "                    new_state = 0.\n",
    "                    superSet.add((weight_name, row, column, 1, 0))\n",
    "\n",
    "                    # Injecting multiple faults (three)\n",
    "                    st[row][column] = new_state\n",
    "                    st[row][column1] = new_state\n",
    "                    st[row][column2] = new_state\n",
    "                    \n",
    "                    [populateResults1(idx.item(), batch_id, 1, 0, w_images, row, column, weight_name) \\\n",
    "                     for idx in getResult(img, label, model, state_dict)]\n",
    "                    \n",
    "                    new_state = neg\n",
    "                    superSet.add((weight_name, row, column, 1, -1))\n",
    "\n",
    "                    # Injecting multiple faults (three)\n",
    "                    st[row][column] = new_state\n",
    "                    st[row][column1] = new_state\n",
    "                    st[row][column2] = new_state\n",
    "                        \n",
    "                    [populateResults1(idx.item(), batch_id, 1, -1, w_images, row, column, weight_name) \\\n",
    "                     for idx in getResult(img, label, model, state_dict)]\n",
    "                    st[row][column] = org_val\n",
    "                    st[row][column1] = org_val\n",
    "                    st[row][column2] = org_val\n",
    "\n",
    "\n",
    "                elif (un > 0) and (un == org_val_v1):  ################# Positive ###############################################################\n",
    "                  org_val = un\n",
    "                  old_state = un\n",
    "                  new_state = 0.\n",
    "                  superSet.add((weight_name, row, column, 1, 0))\n",
    "\n",
    "                  # Injecting multiple faults (three)\n",
    "                  st[row][column] = new_state\n",
    "                  st[row][column1] = new_state\n",
    "                  st[row][column2] = new_state\n",
    "                    \n",
    "                  [populateResults1(idx.item(), batch_id, 1, 0, w_images, row, column, weight_name) \\\n",
    "                   for idx in getResult(img, label, model, state_dict)]\n",
    "                  \n",
    "                  st[row][column] = org_val\n",
    "                  st[row][column1] = org_val\n",
    "                  st[row][column2] = org_val\n",
    "\n",
    "\n",
    "    batch_id += 1\n",
    "\n",
    "\n",
    "    # Fetching locations and total number of images\n",
    "    ################################################\n",
    "\n",
    "    print(f\"Total number of elements in superSet is : {len(superSet)}.\",'\\n')\n",
    "\n",
    "    # joblib.dump(w_images, open('w_images_'+str(batch_id)+'.pkl','wb')) \n",
    "\n",
    "    image_locCounts = [] # (image_id,locationCounts)\n",
    "\n",
    "    for img in w_images.keys():\n",
    "        locCounts = len(w_images[img]['location'])\n",
    "        image_locCounts.append((locCounts,img))\n",
    "            \n",
    "    image_locCounts.sort(reverse=True)\n",
    "\n",
    "\n",
    "    netSetOfImages = []\n",
    "    progressingSet = []\n",
    "    max_locCounts_img = image_locCounts[0][1] # An image that has encompassed maximum locations.\n",
    "\n",
    "    netSetOfImages.append(max_locCounts_img)\n",
    "    progressingSet += set(w_images[max_locCounts_img]['location'])\n",
    "\n",
    "\n",
    "    for i in range(1,len(image_locCounts)):\n",
    "        img = image_locCounts[i][1]\n",
    "        locs = w_images[img]['location']\n",
    "\n",
    "        if set(locs).issubset(progressingSet):\n",
    "            pass\n",
    "        else:\n",
    "            progressingSet += list(set(locs) - set(progressingSet))\n",
    "            netSetOfImages.append(img)\n",
    "\n",
    "\n",
    "    if set(superSet).issubset(progressingSet):\n",
    "        print(\"All match done!\")\n",
    "    else:\n",
    "        print(\"There are {} locations that did not match.\".format(len(set(superSet) - set(progressingSet))))\n",
    "\n",
    "    print()\n",
    "    print(\"Total number of locations {} are encompassed out of {}.\".format(len(set(progressingSet)), len(superSet)))\n",
    "\n",
    "\n",
    "    print(\"Total number of elements in a weight matrix is \", len(superSet))\n",
    "\n",
    "    print(\"Total number of images fetched \",len(netSetOfImages),'\\n')\n",
    "\n",
    "    fetchimg_totImg_coverage_totCov.append(( len(netSetOfImages), len(set(progressingSet)), len(superSet) ))\n",
    "\n",
    "    # joblib.dump(fetchimg_totImg_coverage_totCov, open('res'+str(numImgs)+'.pkl','wb')) \n",
    "\n",
    "    # Superset \n",
    "    # joblib.dump(superSet, open('superSet.pkl','wb'))    \n",
    "\n",
    "                \n",
    "      \n",
    "    print(\"Output \\n\")\n",
    "    print(fetchimg_totImg_coverage_totCov)\n",
    "\n",
    "    #joblib.dump(fetchimg_totImg_coverage_totCov, open('op_128.pkl', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1656524256662,
     "user": {
      "displayName": "Abhishek kumar Mishra",
      "userId": "13515131134791977528"
     },
     "user_tz": 240
    },
    "id": "3k3urviiHpjk",
    "outputId": "3754f87a-5c42-489e-db5e-c6f04655b31f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.875072231853136"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fault coverage (New : 2 templates)\n",
    "fetchimg_totImg_coverage_totCov[-1][1]/fetchimg_totImg_coverage_totCov[-1][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iZh-TmJjKULD"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyObb93lHmWrikoJpKnbivT3",
   "collapsed_sections": [],
   "mount_file_id": "1grm5b8hZdyb7iq-ptiWyVMq32j19q4BW",
   "name": "3_Main_ResNet-18_monotonic_decreasing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1344a746f0a048b49330bfee4223e435": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9560a152fd3e4d849e775d9874a4b7d7",
      "placeholder": "",
      "style": "IPY_MODEL_b184f9fe6eb94bef90cbf673609bdc25",
      "value": "100%"
     }
    },
    "2b8cd6af10e94ffb96cbf9fbc83d0d27": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5916b897377e46329fba3cc7c18f73a8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "61b4c885e6aa46efa9e0608ecf3c989b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "92995c4a616043a2a0051788bd0b6dec": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9560a152fd3e4d849e775d9874a4b7d7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b184f9fe6eb94bef90cbf673609bdc25": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cb761cf8573a492bb9f01913f0f46af3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d32ce48a5a324ba280933f9084d3683a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cb761cf8573a492bb9f01913f0f46af3",
      "placeholder": "",
      "style": "IPY_MODEL_61b4c885e6aa46efa9e0608ecf3c989b",
      "value": " 44.7M/44.7M [00:00&lt;00:00, 190MB/s]"
     }
    },
    "da0bb411a7fa4d0aac1333cd9ea1b3e9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_92995c4a616043a2a0051788bd0b6dec",
      "max": 46830571,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2b8cd6af10e94ffb96cbf9fbc83d0d27",
      "value": 46830571
     }
    },
    "e4fe238396d44cb58e4d2a359e162e55": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1344a746f0a048b49330bfee4223e435",
       "IPY_MODEL_da0bb411a7fa4d0aac1333cd9ea1b3e9",
       "IPY_MODEL_d32ce48a5a324ba280933f9084d3683a"
      ],
      "layout": "IPY_MODEL_5916b897377e46329fba3cc7c18f73a8"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
