{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd CIFAR-10/ResNet-18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "KXMJB6isiRyk"
   },
   "outputs": [],
   "source": [
    "import numpy as np, joblib\n",
    "import sys, os, random\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle, gzip\n",
    "from tqdm import tqdm,tqdm_notebook\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0ok6X5PjkKFX"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "seed_num = 1000\n",
    "\n",
    "# For reproducibility when you run the file with .py\n",
    "torch.cuda.is_available()\n",
    "torch.manual_seed(seed_num)\n",
    "torch.cuda.manual_seed(seed_num)\n",
    "np.random.seed(seed_num)\n",
    "random.seed(seed_num)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "torch.backends.cudnn.deterministic =True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "10agOYLDkidx",
    "outputId": "52f13127-c6f5-43c4-9d9a-872c7dc60b8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(seed_num)\n",
    "torch.cuda.manual_seed(seed_num)\n",
    "np.random.seed(seed_num)\n",
    "random.seed(seed_num)\n",
    "\n",
    "# Normalization\n",
    "train_transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))]) \n",
    "test_transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
    "\n",
    "# Splitting the training and test datasets\n",
    "train_data = datasets.CIFAR10(os.getcwd(), train=True,\n",
    "                              download=True, transform=train_transform)\n",
    "test_data = datasets.CIFAR10(os.getcwd(), train=False,\n",
    "                             download=True, transform=test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "5GnCuQ91lCty"
   },
   "outputs": [],
   "source": [
    "# Split the training set indices into training and validation set indices using 80:20 ratio\n",
    "np.random.seed(seed_num)\n",
    "len_trainset = len(train_data)\n",
    "index_list = list(range(len_trainset))\n",
    "np.random.shuffle(index_list)\n",
    "split_index = 40000\n",
    "train_indices, valid_indices =  index_list[:split_index], index_list[split_index:]\n",
    "\n",
    "# Creating Samplers for training and validation set using the indices\n",
    "np.random.seed(seed_num)\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(valid_indices)\n",
    "\n",
    "torch.manual_seed(seed_num)\n",
    "\n",
    "train_iterator = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\n",
    "val_iterator = DataLoader(train_data, batch_size=batch_size, sampler=valid_sampler)\n",
    "test_iterator = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "hNkLu8CWSUmj"
   },
   "outputs": [],
   "source": [
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "1e27c11f831443e987cc77068fca71c4",
      "abd74acda0794e199ab77a4807078112",
      "63f4a074909f40159f9cdfed7776a8b5",
      "e46cb067c2874e199328a6e7053c44ad",
      "8bf5a2f9c6af47fc99794231d85f6be6",
      "c31df19d7d134a0393d202497be264ea",
      "6d5532489ef74deeb7422c57867859b0",
      "c115fd574064461eaa866428253b58d0",
      "9a93212993fa4d11aafb6a7a62827a77",
      "4cb5c328c5e7439ca898fcc8219c0288",
      "22f063e483974dc0963873ee184c6d5c"
     ]
    },
    "id": "4D-XvGNzlJn_",
    "outputId": "714647ab-e032-4260-8c13-45253fca73ff"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e27c11f831443e987cc77068fca71c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/44.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ResNet-18 model\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        base = models.resnet18(pretrained=True)\n",
    "        self.base = nn.Sequential(*list(base.children())[:-1])\n",
    "        in_features = base.fc.in_features\n",
    "        self.drop = nn.Dropout()\n",
    "        self.final = nn.Linear(in_features,10)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.base(x)\n",
    "        x = self.drop(x.view(-1,self.final.in_features))\n",
    "        return self.final(x)\n",
    "    \n",
    "model = Model().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W-97MR9BlJrW",
    "outputId": "5f368b16-417d-46d0-a78b-2dcd01777565"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights done !\n",
      "Model:\n",
      " Model(\n",
      "  (base): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (4): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (6): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (7): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  )\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (final): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Loading the weights of ternary model \n",
    "model = torch.load(\"Main_ResNet18_cifar10_Quant.pt\").cuda()\n",
    "print(\"Loading weights done !\")\n",
    "\n",
    "# Summary\n",
    "print(\"Model:\\n\",model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A8PBc3NYlJuX",
    "outputId": "3cfb15c7-b8e7-40b6-d1d0-da8cb085890b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['base.0.weight', 'base.1.bias', 'base.1.weight', 'base.4.0.conv1.weight', 'base.4.0.bn1.bias', 'base.4.0.bn1.weight', 'base.4.0.conv2.weight', 'base.4.0.bn2.bias', 'base.4.0.bn2.weight', 'base.4.1.conv1.weight', 'base.4.1.bn1.bias', 'base.4.1.bn1.weight', 'base.4.1.conv2.weight', 'base.4.1.bn2.bias', 'base.4.1.bn2.weight', 'base.5.0.conv1.weight', 'base.5.0.bn1.bias', 'base.5.0.bn1.weight', 'base.5.0.conv2.weight', 'base.5.0.bn2.bias', 'base.5.0.bn2.weight', 'base.5.0.downsample.0.weight', 'base.5.0.downsample.1.bias', 'base.5.0.downsample.1.weight', 'base.5.1.conv1.weight', 'base.5.1.bn1.bias', 'base.5.1.bn1.weight', 'base.5.1.conv2.weight', 'base.5.1.bn2.bias', 'base.5.1.bn2.weight', 'base.6.0.conv1.weight', 'base.6.0.bn1.bias', 'base.6.0.bn1.weight', 'base.6.0.conv2.weight', 'base.6.0.bn2.bias', 'base.6.0.bn2.weight', 'base.6.0.downsample.0.weight', 'base.6.0.downsample.1.bias', 'base.6.0.downsample.1.weight', 'base.6.1.conv1.weight', 'base.6.1.bn1.bias', 'base.6.1.bn1.weight', 'base.6.1.conv2.weight', 'base.6.1.bn2.bias', 'base.6.1.bn2.weight', 'base.7.0.conv1.weight', 'base.7.0.bn1.bias', 'base.7.0.bn1.weight', 'base.7.0.conv2.weight', 'base.7.0.bn2.bias', 'base.7.0.bn2.weight', 'base.7.0.downsample.0.weight', 'base.7.0.downsample.1.bias', 'base.7.0.downsample.1.weight', 'base.7.1.conv1.weight', 'base.7.1.bn1.bias', 'base.7.1.bn1.weight', 'base.7.1.conv2.weight', 'base.7.1.bn2.bias', 'base.7.1.bn2.weight', 'final.bias', 'final.weight']\n"
     ]
    }
   ],
   "source": [
    "# Layer names\n",
    "layer_name = [n for n, p in model.named_parameters()]\n",
    "print(layer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7V_swsrplJxP",
    "outputId": "590e9fbf-0715-47a5-ea04-a187f218a882"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base.0.weight tensor(1449, device='cuda:0')\n",
      "base.1.bias tensor(52, device='cuda:0')\n",
      "base.1.weight tensor(52, device='cuda:0')\n",
      "base.4.0.conv1.weight tensor(1856, device='cuda:0')\n",
      "base.4.0.bn1.bias tensor(52, device='cuda:0')\n",
      "base.4.0.bn1.weight tensor(52, device='cuda:0')\n",
      "base.4.0.conv2.weight tensor(2120, device='cuda:0')\n",
      "base.4.0.bn2.bias tensor(53, device='cuda:0')\n",
      "base.4.0.bn2.weight tensor(53, device='cuda:0')\n",
      "base.4.1.conv1.weight tensor(1952, device='cuda:0')\n",
      "base.4.1.bn1.bias tensor(52, device='cuda:0')\n",
      "base.4.1.bn1.weight tensor(52, device='cuda:0')\n",
      "base.4.1.conv2.weight tensor(2319, device='cuda:0')\n",
      "base.4.1.bn2.bias tensor(53, device='cuda:0')\n",
      "base.4.1.bn2.weight tensor(52, device='cuda:0')\n",
      "base.5.0.conv1.weight tensor(1304, device='cuda:0')\n",
      "base.5.0.bn1.bias tensor(104, device='cuda:0')\n",
      "base.5.0.bn1.weight tensor(104, device='cuda:0')\n",
      "base.5.0.conv2.weight tensor(1404, device='cuda:0')\n",
      "base.5.0.bn2.bias tensor(107, device='cuda:0')\n",
      "base.5.0.bn2.weight tensor(105, device='cuda:0')\n",
      "base.5.0.downsample.0.weight tensor(1320, device='cuda:0')\n",
      "base.5.0.downsample.1.bias tensor(107, device='cuda:0')\n",
      "base.5.0.downsample.1.weight tensor(104, device='cuda:0')\n",
      "base.5.1.conv1.weight tensor(1251, device='cuda:0')\n",
      "base.5.1.bn1.bias tensor(104, device='cuda:0')\n",
      "base.5.1.bn1.weight tensor(104, device='cuda:0')\n",
      "base.5.1.conv2.weight tensor(1570, device='cuda:0')\n",
      "base.5.1.bn2.bias tensor(104, device='cuda:0')\n",
      "base.5.1.bn2.weight tensor(104, device='cuda:0')\n",
      "base.6.0.conv1.weight tensor(2643, device='cuda:0')\n",
      "base.6.0.bn1.bias tensor(206, device='cuda:0')\n",
      "base.6.0.bn1.weight tensor(207, device='cuda:0')\n",
      "base.6.0.conv2.weight tensor(2370, device='cuda:0')\n",
      "base.6.0.bn2.bias tensor(212, device='cuda:0')\n",
      "base.6.0.bn2.weight tensor(207, device='cuda:0')\n",
      "base.6.0.downsample.0.weight tensor(1968, device='cuda:0')\n",
      "base.6.0.downsample.1.bias tensor(212, device='cuda:0')\n",
      "base.6.0.downsample.1.weight tensor(208, device='cuda:0')\n",
      "base.6.1.conv1.weight tensor(1479, device='cuda:0')\n",
      "base.6.1.bn1.bias tensor(207, device='cuda:0')\n",
      "base.6.1.bn1.weight tensor(207, device='cuda:0')\n",
      "base.6.1.conv2.weight tensor(1173, device='cuda:0')\n",
      "base.6.1.bn2.bias tensor(208, device='cuda:0')\n",
      "base.6.1.bn2.weight tensor(207, device='cuda:0')\n",
      "base.7.0.conv1.weight tensor(1999, device='cuda:0')\n",
      "base.7.0.bn1.bias tensor(425, device='cuda:0')\n",
      "base.7.0.bn1.weight tensor(415, device='cuda:0')\n",
      "base.7.0.conv2.weight tensor(1319, device='cuda:0')\n",
      "base.7.0.bn2.bias tensor(468, device='cuda:0')\n",
      "base.7.0.bn2.weight tensor(415, device='cuda:0')\n",
      "base.7.0.downsample.0.weight tensor(1491, device='cuda:0')\n",
      "base.7.0.downsample.1.bias tensor(468, device='cuda:0')\n",
      "base.7.0.downsample.1.weight tensor(417, device='cuda:0')\n",
      "base.7.1.conv1.weight tensor(871, device='cuda:0')\n",
      "base.7.1.bn1.bias tensor(422, device='cuda:0')\n",
      "base.7.1.bn1.weight tensor(415, device='cuda:0')\n",
      "base.7.1.conv2.weight tensor(766, device='cuda:0')\n",
      "base.7.1.bn2.bias tensor(451, device='cuda:0')\n",
      "base.7.1.bn2.weight tensor(415, device='cuda:0')\n",
      "final.bias tensor(10, device='cuda:0')\n",
      "final.weight tensor(3418, device='cuda:0')\n",
      "Total Parameters: tensor(44014, device='cuda:0') \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Total number of ternary weights (+w, -w)\n",
    "totalParams = 0\n",
    "for i in layer_name:\n",
    "  print(i,(model.state_dict()[i] !=0).sum())\n",
    "  totalParams +=  (model.state_dict()[i] !=0).sum()\n",
    "    \n",
    "print(\"Total Parameters:\",totalParams, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0uPbBVUKlJ0X",
    "outputId": "9c730ed8-d000-4200-b1e1-6c0cd7e8522b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number Of Images Tested = 10000\n",
      "\n",
      "Model Test Accuracy = 0.5374\n"
     ]
    }
   ],
   "source": [
    "# Model's performance on test set\n",
    "\n",
    "correct_count, all_count = 0, 0\n",
    "model.eval()\n",
    "for images,labels in test_iterator:\n",
    "      for image,label in zip(images,labels):\n",
    "        if torch.cuda.is_available():\n",
    "            img = image.cuda()\n",
    "            lab = label.cuda()\n",
    "            img = img[None,].type('torch.cuda.FloatTensor')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_ = model(img) \n",
    "\n",
    "        pred_label = output_.argmax()\n",
    "\n",
    "        if(pred_label.item()==lab.item()):\n",
    "          correct_count += 1\n",
    "        all_count += 1\n",
    "\n",
    "print(\"Number Of Images Tested =\", all_count)\n",
    "print(\"\\nModel Test Accuracy =\", (correct_count/(all_count)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RvZCWE-QlJ6U",
    "outputId": "95ab359e-cddb-4263-c349-9755297541e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base.0.weight hidden layer dimension torch.Size([64, 3, 7, 7])\n",
      "Unique values of weight in base.0.weight th hidden layer :  tensor([-0.9980,  0.0000,  0.9980], device='cuda:0')\n",
      "\n",
      "base.1.bias hidden layer dimension torch.Size([64])\n",
      "Unique values of weight in base.1.bias th hidden layer :  tensor([-1.0023,  0.0000,  1.0049], device='cuda:0')\n",
      "\n",
      "base.1.weight hidden layer dimension torch.Size([64])\n",
      "Unique values of weight in base.1.weight th hidden layer :  tensor([0.0000, 1.0063], device='cuda:0')\n",
      "\n",
      "base.4.0.conv1.weight hidden layer dimension torch.Size([64, 64, 3, 3])\n",
      "Unique values of weight in base.4.0.conv1.weight th hidden layer :  tensor([-0.9993,  0.0000,  0.9993], device='cuda:0')\n",
      "\n",
      "base.4.0.bn1.bias hidden layer dimension torch.Size([64])\n",
      "Unique values of weight in base.4.0.bn1.bias th hidden layer :  tensor([-1.0043,  0.0000,  1.0005], device='cuda:0')\n",
      "\n",
      "base.4.0.bn1.weight hidden layer dimension torch.Size([64])\n",
      "Unique values of weight in base.4.0.bn1.weight th hidden layer :  tensor([0.0000, 1.0030], device='cuda:0')\n",
      "\n",
      "base.4.0.conv2.weight hidden layer dimension torch.Size([64, 64, 3, 3])\n",
      "Unique values of weight in base.4.0.conv2.weight th hidden layer :  tensor([-0.9980,  0.0000,  0.9980], device='cuda:0')\n",
      "\n",
      "base.4.0.bn2.bias hidden layer dimension torch.Size([64])\n",
      "Unique values of weight in base.4.0.bn2.bias th hidden layer :  tensor([-1.0084,  0.0000,  1.0035], device='cuda:0')\n",
      "\n",
      "base.4.0.bn2.weight hidden layer dimension torch.Size([64])\n",
      "Unique values of weight in base.4.0.bn2.weight th hidden layer :  tensor([0.0000, 0.9990], device='cuda:0')\n",
      "\n",
      "base.4.1.conv1.weight hidden layer dimension torch.Size([64, 64, 3, 3])\n",
      "Unique values of weight in base.4.1.conv1.weight th hidden layer :  tensor([-0.9982,  0.0000,  0.9982], device='cuda:0')\n",
      "\n",
      "base.4.1.bn1.bias hidden layer dimension torch.Size([64])\n",
      "Unique values of weight in base.4.1.bn1.bias th hidden layer :  tensor([-1.0036,  0.0000,  1.0017], device='cuda:0')\n",
      "\n",
      "base.4.1.bn1.weight hidden layer dimension torch.Size([64])\n",
      "Unique values of weight in base.4.1.bn1.weight th hidden layer :  tensor([0.0000, 1.0017], device='cuda:0')\n",
      "\n",
      "base.4.1.conv2.weight hidden layer dimension torch.Size([64, 64, 3, 3])\n",
      "Unique values of weight in base.4.1.conv2.weight th hidden layer :  tensor([-1.0003,  0.0000,  1.0003], device='cuda:0')\n",
      "\n",
      "base.4.1.bn2.bias hidden layer dimension torch.Size([64])\n",
      "Unique values of weight in base.4.1.bn2.bias th hidden layer :  tensor([-1.0087,  0.0000,  0.9983], device='cuda:0')\n",
      "\n",
      "base.4.1.bn2.weight hidden layer dimension torch.Size([64])\n",
      "Unique values of weight in base.4.1.bn2.weight th hidden layer :  tensor([0.0000, 0.9972], device='cuda:0')\n",
      "\n",
      "base.5.0.conv1.weight hidden layer dimension torch.Size([128, 64, 3, 3])\n",
      "Unique values of weight in base.5.0.conv1.weight th hidden layer :  tensor([-1.0036,  0.0000,  1.0036], device='cuda:0')\n",
      "\n",
      "base.5.0.bn1.bias hidden layer dimension torch.Size([128])\n",
      "Unique values of weight in base.5.0.bn1.bias th hidden layer :  tensor([-1.0057,  0.0000,  1.0006], device='cuda:0')\n",
      "\n",
      "base.5.0.bn1.weight hidden layer dimension torch.Size([128])\n",
      "Unique values of weight in base.5.0.bn1.weight th hidden layer :  tensor([0.0000, 1.0038], device='cuda:0')\n",
      "\n",
      "base.5.0.conv2.weight hidden layer dimension torch.Size([128, 128, 3, 3])\n",
      "Unique values of weight in base.5.0.conv2.weight th hidden layer :  tensor([-1.0007,  0.0000,  1.0007], device='cuda:0')\n",
      "\n",
      "base.5.0.bn2.bias hidden layer dimension torch.Size([128])\n",
      "Unique values of weight in base.5.0.bn2.bias th hidden layer :  tensor([-1.0034,  0.0000,  1.0027], device='cuda:0')\n",
      "\n",
      "base.5.0.bn2.weight hidden layer dimension torch.Size([128])\n",
      "Unique values of weight in base.5.0.bn2.weight th hidden layer :  tensor([0.0000, 0.9999], device='cuda:0')\n",
      "\n",
      "base.5.0.downsample.0.weight hidden layer dimension torch.Size([128, 64, 1, 1])\n",
      "Unique values of weight in base.5.0.downsample.0.weight th hidden layer :  tensor([-0.9982,  0.0000,  0.9982], device='cuda:0')\n",
      "\n",
      "base.5.0.downsample.1.bias hidden layer dimension torch.Size([128])\n",
      "Unique values of weight in base.5.0.downsample.1.bias th hidden layer :  tensor([-1.0034,  0.0000,  1.0027], device='cuda:0')\n",
      "\n",
      "base.5.0.downsample.1.weight hidden layer dimension torch.Size([128])\n",
      "Unique values of weight in base.5.0.downsample.1.weight th hidden layer :  tensor([0.0000, 1.0067], device='cuda:0')\n",
      "\n",
      "base.5.1.conv1.weight hidden layer dimension torch.Size([128, 128, 3, 3])\n",
      "Unique values of weight in base.5.1.conv1.weight th hidden layer :  tensor([-1.0008,  0.0000,  1.0008], device='cuda:0')\n",
      "\n",
      "base.5.1.bn1.bias hidden layer dimension torch.Size([128])\n",
      "Unique values of weight in base.5.1.bn1.bias th hidden layer :  tensor([-1.0060,  0.0000], device='cuda:0')\n",
      "\n",
      "base.5.1.bn1.weight hidden layer dimension torch.Size([128])\n",
      "Unique values of weight in base.5.1.bn1.weight th hidden layer :  tensor([0.0000, 1.0060], device='cuda:0')\n",
      "\n",
      "base.5.1.conv2.weight hidden layer dimension torch.Size([128, 128, 3, 3])\n",
      "Unique values of weight in base.5.1.conv2.weight th hidden layer :  tensor([-0.9999,  0.0000,  0.9999], device='cuda:0')\n",
      "\n",
      "base.5.1.bn2.bias hidden layer dimension torch.Size([128])\n",
      "Unique values of weight in base.5.1.bn2.bias th hidden layer :  tensor([-1.0100,  0.0000,  1.0002], device='cuda:0')\n",
      "\n",
      "base.5.1.bn2.weight hidden layer dimension torch.Size([128])\n",
      "Unique values of weight in base.5.1.bn2.weight th hidden layer :  tensor([0.0000, 0.9948], device='cuda:0')\n",
      "\n",
      "base.6.0.conv1.weight hidden layer dimension torch.Size([256, 128, 3, 3])\n",
      "Unique values of weight in base.6.0.conv1.weight th hidden layer :  tensor([-0.9990,  0.0000,  0.9990], device='cuda:0')\n",
      "\n",
      "base.6.0.bn1.bias hidden layer dimension torch.Size([256])\n",
      "Unique values of weight in base.6.0.bn1.bias th hidden layer :  tensor([-1.0127,  0.0000,  0.9936], device='cuda:0')\n",
      "\n",
      "base.6.0.bn1.weight hidden layer dimension torch.Size([256])\n",
      "Unique values of weight in base.6.0.bn1.weight th hidden layer :  tensor([0.0000, 1.0134], device='cuda:0')\n",
      "\n",
      "base.6.0.conv2.weight hidden layer dimension torch.Size([256, 256, 3, 3])\n",
      "Unique values of weight in base.6.0.conv2.weight th hidden layer :  tensor([-1.0042,  0.0000,  1.0041], device='cuda:0')\n",
      "\n",
      "base.6.0.bn2.bias hidden layer dimension torch.Size([256])\n",
      "Unique values of weight in base.6.0.bn2.bias th hidden layer :  tensor([-1.0014,  0.0000,  1.0028], device='cuda:0')\n",
      "\n",
      "base.6.0.bn2.weight hidden layer dimension torch.Size([256])\n",
      "Unique values of weight in base.6.0.bn2.weight th hidden layer :  tensor([0.0000, 1.0063], device='cuda:0')\n",
      "\n",
      "base.6.0.downsample.0.weight hidden layer dimension torch.Size([256, 128, 1, 1])\n",
      "Unique values of weight in base.6.0.downsample.0.weight th hidden layer :  tensor([-0.9986,  0.0000,  0.9986], device='cuda:0')\n",
      "\n",
      "base.6.0.downsample.1.bias hidden layer dimension torch.Size([256])\n",
      "Unique values of weight in base.6.0.downsample.1.bias th hidden layer :  tensor([-1.0014,  0.0000,  1.0028], device='cuda:0')\n",
      "\n",
      "base.6.0.downsample.1.weight hidden layer dimension torch.Size([256])\n",
      "Unique values of weight in base.6.0.downsample.1.weight th hidden layer :  tensor([-1.0047,  0.0000,  0.9975], device='cuda:0')\n",
      "\n",
      "base.6.1.conv1.weight hidden layer dimension torch.Size([256, 256, 3, 3])\n",
      "Unique values of weight in base.6.1.conv1.weight th hidden layer :  tensor([-0.9996,  0.0000,  0.9996], device='cuda:0')\n",
      "\n",
      "base.6.1.bn1.bias hidden layer dimension torch.Size([256])\n",
      "Unique values of weight in base.6.1.bn1.bias th hidden layer :  tensor([-1.0039,  0.0000], device='cuda:0')\n",
      "\n",
      "base.6.1.bn1.weight hidden layer dimension torch.Size([256])\n",
      "Unique values of weight in base.6.1.bn1.weight th hidden layer :  tensor([0.0000, 1.0039], device='cuda:0')\n",
      "\n",
      "base.6.1.conv2.weight hidden layer dimension torch.Size([256, 256, 3, 3])\n",
      "Unique values of weight in base.6.1.conv2.weight th hidden layer :  tensor([-1.0011,  0.0000,  1.0010], device='cuda:0')\n",
      "\n",
      "base.6.1.bn2.bias hidden layer dimension torch.Size([256])\n",
      "Unique values of weight in base.6.1.bn2.bias th hidden layer :  tensor([-1.0052,  0.0000,  1.0001], device='cuda:0')\n",
      "\n",
      "base.6.1.bn2.weight hidden layer dimension torch.Size([256])\n",
      "Unique values of weight in base.6.1.bn2.weight th hidden layer :  tensor([0.0000, 0.9972], device='cuda:0')\n",
      "\n",
      "base.7.0.conv1.weight hidden layer dimension torch.Size([512, 256, 3, 3])\n",
      "Unique values of weight in base.7.0.conv1.weight th hidden layer :  tensor([-0.9972,  0.0000,  0.9971], device='cuda:0')\n",
      "\n",
      "base.7.0.bn1.bias hidden layer dimension torch.Size([512])\n",
      "Unique values of weight in base.7.0.bn1.bias th hidden layer :  tensor([-1.0133,  0.0000,  1.0029], device='cuda:0')\n",
      "\n",
      "base.7.0.bn1.weight hidden layer dimension torch.Size([512])\n",
      "Unique values of weight in base.7.0.bn1.weight th hidden layer :  tensor([0.0000, 1.0119], device='cuda:0')\n",
      "\n",
      "base.7.0.conv2.weight hidden layer dimension torch.Size([512, 512, 3, 3])\n",
      "Unique values of weight in base.7.0.conv2.weight th hidden layer :  tensor([-0.9794,  0.0000,  0.9794], device='cuda:0')\n",
      "\n",
      "base.7.0.bn2.bias hidden layer dimension torch.Size([512])\n",
      "Unique values of weight in base.7.0.bn2.bias th hidden layer :  tensor([-0.9647,  0.0000,  0.9783], device='cuda:0')\n",
      "\n",
      "base.7.0.bn2.weight hidden layer dimension torch.Size([512])\n",
      "Unique values of weight in base.7.0.bn2.weight th hidden layer :  tensor([0.0000, 0.9599], device='cuda:0')\n",
      "\n",
      "base.7.0.downsample.0.weight hidden layer dimension torch.Size([512, 256, 1, 1])\n",
      "Unique values of weight in base.7.0.downsample.0.weight th hidden layer :  tensor([-0.9850,  0.0000,  0.9849], device='cuda:0')\n",
      "\n",
      "base.7.0.downsample.1.bias hidden layer dimension torch.Size([512])\n",
      "Unique values of weight in base.7.0.downsample.1.bias th hidden layer :  tensor([-0.9647,  0.0000,  0.9783], device='cuda:0')\n",
      "\n",
      "base.7.0.downsample.1.weight hidden layer dimension torch.Size([512])\n",
      "Unique values of weight in base.7.0.downsample.1.weight th hidden layer :  tensor([-1.0000,  0.0000,  0.9638], device='cuda:0')\n",
      "\n",
      "base.7.1.conv1.weight hidden layer dimension torch.Size([512, 512, 3, 3])\n",
      "Unique values of weight in base.7.1.conv1.weight th hidden layer :  tensor([-1.0019,  0.0000,  1.0019], device='cuda:0')\n",
      "\n",
      "base.7.1.bn1.bias hidden layer dimension torch.Size([512])\n",
      "Unique values of weight in base.7.1.bn1.bias th hidden layer :  tensor([-1.0207,  0.0000,  0.9879], device='cuda:0')\n",
      "\n",
      "base.7.1.bn1.weight hidden layer dimension torch.Size([512])\n",
      "Unique values of weight in base.7.1.bn1.weight th hidden layer :  tensor([0.0000, 1.0216], device='cuda:0')\n",
      "\n",
      "base.7.1.conv2.weight hidden layer dimension torch.Size([512, 512, 3, 3])\n",
      "Unique values of weight in base.7.1.conv2.weight th hidden layer :  tensor([-1.0151,  0.0000,  1.0143], device='cuda:0')\n",
      "\n",
      "base.7.1.bn2.bias hidden layer dimension torch.Size([512])\n",
      "Unique values of weight in base.7.1.bn2.bias th hidden layer :  tensor([-0.9805,  0.0000,  0.9251], device='cuda:0')\n",
      "\n",
      "base.7.1.bn2.weight hidden layer dimension torch.Size([512])\n",
      "Unique values of weight in base.7.1.bn2.weight th hidden layer :  tensor([0.0000, 0.9946], device='cuda:0')\n",
      "\n",
      "final.bias hidden layer dimension torch.Size([10])\n",
      "Unique values of weight in final.bias th hidden layer :  tensor([-1.0001,  1.0003], device='cuda:0')\n",
      "\n",
      "final.weight hidden layer dimension torch.Size([10, 512])\n",
      "Unique values of weight in final.weight th hidden layer :  tensor([-1.0668,  0.0000,  0.9294], device='cuda:0')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For each layer, model's ternary weights\n",
    "state_dict = model.state_dict()\n",
    "\n",
    "layer_distinct_weights = {}\n",
    "\n",
    "for i in layer_name:\n",
    "  imd = torch.unique(model.state_dict()[i])\n",
    "  print(i+ ' hidden layer dimension', model.state_dict()[i].shape)\n",
    "  print(\"Unique values of weight in \"+ i+ \" th hidden layer : \", imd)\n",
    "  layer_distinct_weights[i] = imd.cpu().numpy().tolist()\n",
    "  print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gDc63tH8h0W_",
    "outputId": "8b6b1147-bbee-44ce-d6ee-532cbef48de8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 3, 32, 32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Single seed normal distribution\n",
    "np.random.seed(seed_num)\n",
    "corr_images1 = []\n",
    "\n",
    "for i in range(4000): \n",
    "  corr_images1.append(1 + 0.5 *np.random.randn(3,32,32))\n",
    "\n",
    "corr_images1 = np.array(corr_images1)\n",
    "corr_images1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "DbvzOfCTkl2s"
   },
   "outputs": [],
   "source": [
    "# Structured Patterns\n",
    "\n",
    "random.seed(seed_num)\n",
    "np.random.seed(seed_num)\n",
    "torch.manual_seed(seed_num)\n",
    "\n",
    "corr_images2 = []\n",
    "a = np.identity(32) * np.random.randn(32)\n",
    "corr_images2.append(a)\n",
    "b = np.fliplr(a.copy()) * np.random.randn(32)\n",
    "corr_images2.append(b)\n",
    "c = (a+b).copy() * np.random.randn(32)\n",
    "corr_images2.append(c)\n",
    "\n",
    "# To apply [1,1] , [0,1], [1,0]\n",
    "cp_file = [a,b,c]\n",
    "\n",
    "# To apply randn\n",
    "a1,b1,c1 = a.copy(), b.copy(), c.copy()\n",
    "cp_file1 = [a1, b1, c1]\n",
    "\n",
    "files = [cp_file, cp_file1]\n",
    "\n",
    "for i in range(700):\n",
    "    \n",
    "    np.random.seed(i)\n",
    "    choose = np.random.randint(0,2)\n",
    "\n",
    "    # Picking row and column indices randomly\n",
    "    r_idx = np.random.randint(0,32)\n",
    "    c_idx = np.random.randint(0,32)\n",
    "\n",
    "    if choose == 0:\n",
    "        \n",
    "        # Selecting file\n",
    "        file = files[0]\n",
    "        \n",
    "        imd_img = file[np.random.randint(0,3)].copy()\n",
    "        \n",
    "        imd_img1 = file[np.random.randint(0,3)]\n",
    "        \n",
    "        opt =[[1,1] , [0,1], [1,0]]\n",
    "        \n",
    "        if np.random.randint(0,2) == 0:\n",
    "        \n",
    "            imd_img[r_idx,:] = np.array(opt[np.random.randint(0,3)] *16) \n",
    "            corr_images2.append(imd_img)\n",
    "\n",
    "            imd_img[:,c_idx] = np.array(opt[np.random.randint(0,3)] *16) \n",
    "            corr_images2.append(imd_img)\n",
    "\n",
    "        else:\n",
    "            imd_img1[r_idx,:] = np.array(opt[np.random.randint(0,3)] *16) \n",
    "            corr_images2.append(imd_img1)\n",
    "\n",
    "            imd_img1[:,c_idx] = np.array(opt[np.random.randint(0,3)] *16) \n",
    "            corr_images2.append(imd_img1)\n",
    "            \n",
    "\n",
    "    else:\n",
    "        \n",
    "        # Selecting file\n",
    "        file = files[1]\n",
    "        \n",
    "        imd_img = file[np.random.randint(0,3)].copy()\n",
    "        \n",
    "        imd_img1 = file[np.random.randint(0,3)]\n",
    "        \n",
    "        opt =[[1,1] , [0,1], [1,0]]\n",
    "        \n",
    "        if np.random.randint(0,2) == 0:\n",
    "        \n",
    "            imd_img[r_idx,:] = np.array(opt[np.random.randint(0,3)] *16) * np.random.randn(32)\n",
    "            corr_images2.append(imd_img)\n",
    "\n",
    "            imd_img[:,c_idx] = np.array(opt[np.random.randint(0,3)] *16) * np.random.randn(32)\n",
    "            corr_images2.append(imd_img)\n",
    "\n",
    "        else:\n",
    "            imd_img1[r_idx,:] = np.array(opt[np.random.randint(0,3)] *16) * np.random.randn(32)\n",
    "            corr_images2.append(imd_img1)\n",
    "\n",
    "            imd_img1[:,c_idx] = np.array(opt[np.random.randint(0,3)] *16) * np.random.randn(32)\n",
    "            corr_images2.append(imd_img1)\n",
    "    \n",
    "corr_images2 = np.array(corr_images2).reshape(len(corr_images2),1,32,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tN10r0aWkl8l",
    "outputId": "22a84db2-90fb-4b27-a887-dbf7cf9b0053"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(369, 1, 32, 32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_images2 = np.unique(corr_images2, axis=0)\n",
    "len(corr_images2)\n",
    "corr_images2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DjCK286akmCt",
    "outputId": "77d8e3dd-afa1-419b-ef7c-ce86218b4a57"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(369, 3, 32, 32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_images2_ = []\n",
    "for img in corr_images2:\n",
    "  corr_images2_.append(np.vstack([img, img*0.001, img*0.002]))\n",
    "corr_images2 = np.array(corr_images2_)\n",
    "corr_images2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KnIwHKPtkmOY",
    "outputId": "1c48a4e9-9735-4929-a1b6-bfbc9892a16f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 3, 32, 32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Augmentation on structured images\n",
    "\n",
    "random.seed(seed_num)\n",
    "np.random.seed(seed_num)\n",
    "torch.manual_seed(seed_num)\n",
    "\n",
    "corr_images2_1 = []\n",
    "\n",
    "for img in corr_images2:\n",
    "\n",
    "    for angle in range(5,361,25):\n",
    "        transform_st = transforms.Compose([transforms.ToTensor(), transforms.RandomRotation(angle), \\\n",
    "                                             transforms.RandomHorizontalFlip(0.40), transforms.Normalize([0.2860,],[0.3205,])]) \n",
    "        corr_images2_1.append(transform_st(img).permute(1,0,2).numpy())\n",
    "        \n",
    "        transform_st1 = transforms.Compose([transforms.ToTensor(), transforms.RandomRotation(angle), transforms.Normalize([0.2860,],[0.3205,])]) \n",
    "        corr_images2_1.append(transform_st1(img).permute(1,0,2).numpy())\n",
    "        \n",
    "        transform_st2 = transforms.Compose([transforms.ToTensor(),transforms.RandomHorizontalFlip(0.35), transforms.Normalize([0.2860,],[0.3205,])]) \n",
    "        corr_images2_1.append(transform_st2(img).permute(1,0,2).numpy())\n",
    "\n",
    "        transform_st3 = transforms.Compose([transforms.ToTensor(),transforms.RandomRotation(angle), transforms.RandomVerticalFlip(0.70),\n",
    "                                    transforms.RandomAffine(angle),  transforms.Normalize([0.2860,],[0.3205,])]) \n",
    "        corr_images2_1.append(transform_st3(img).permute(1,0,2).numpy())\n",
    "\n",
    "        transform_st4 = transforms.Compose([transforms.ToTensor(), transforms.RandomVerticalFlip(0.25),\n",
    "                                        transforms.RandomAffine(angle), transforms.Normalize([0.2860,],[0.3205,])]) \n",
    "        corr_images2_1.append(transform_st4(img).permute(1,0,2).numpy())\n",
    "\n",
    "        transform_st5 = transforms.Compose([transforms.ToTensor(), transforms.RandomVerticalFlip(0.45),transforms.Normalize([0.2860,],[0.3205,])]) \n",
    "        corr_images2_1.append(transform_st5(img).permute(1,0,2).numpy())\n",
    "\n",
    "        transform_st6 = transforms.Compose([transforms.ToTensor(), transforms.RandomRotation(angle), \\\n",
    "                                              transforms.RandomVerticalFlip(0.35),transforms.Normalize([0.2860,],[0.3205,])]) \n",
    "        corr_images2_1.append(transform_st6(img).permute(1,0,2).numpy())\n",
    "\n",
    "        transform_st7 = transforms.Compose([transforms.ToTensor(),transforms.RandomHorizontalFlip(0.65), transforms.RandomAffine(angle),\\\n",
    "                                            transforms.Normalize([0.2860,],[0.3205,])]) \n",
    "        corr_images2_1.append(transform_st7(img).permute(1,0,2).numpy())\n",
    "\n",
    "        transform_st8 = transforms.Compose([transforms.ToTensor(),transforms.RandomRotation(angle), transforms.RandomAffine(angle),\\\n",
    "                                            transforms.Normalize([0.2860,],[0.3205,])]) \n",
    "        corr_images2_1.append(transform_st8(img).permute(1,0,2).numpy())\n",
    "\n",
    "\n",
    "        transform_st9 = transforms.Compose([transforms.ToTensor(),transforms.RandomRotation(angle), transforms.RandomHorizontalFlip(0.75),\n",
    "                                            transforms.RandomAffine(angle),transforms.Normalize([0.2860,],[0.3205,])]) \n",
    "        corr_images2_1.append(transform_st9(img).permute(1,2,0).numpy())\n",
    "            \n",
    "\n",
    "corr_images2_1 += corr_images2.tolist()\n",
    "corr_images2_1 = np.unique(corr_images2_1, axis=0)\n",
    "idx = np.random.permutation(len(corr_images2_1))[:3000]\n",
    "corr_images2 = corr_images2_1[idx]\n",
    "corr_images2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "zs-Yuz9clFVO"
   },
   "outputs": [],
   "source": [
    "#############\n",
    "# Pixel taking randomly from 8 bits (0-255) to generate images\n",
    "\n",
    "random.seed(seed_num)\n",
    "np.random.seed(seed_num)\n",
    "torch.manual_seed(seed_num)\n",
    "\n",
    "corr_images1_ = []\n",
    "\n",
    "for i in range(400):\n",
    "    img = np.random.randint(0, 256, (3,32,32)).astype('float')/255.\n",
    "    for angle in range(5,361,25):\n",
    "        transform_st = transforms.Compose([transforms.ToTensor(), transforms.RandomRotation(angle), \\\n",
    "                                             transforms.RandomHorizontalFlip(0.25), transforms.Normalize([0.2860,],[0.3205,])]) \n",
    "        corr_images1_.append(transform_st(img).permute(1,0,2).numpy())\n",
    "        \n",
    "        transform_st1 = transforms.Compose([transforms.ToTensor(), transforms.RandomRotation(angle), transforms.Normalize([0.2860,],[0.3205,])]) \n",
    "        corr_images1_.append(transform_st1(img).permute(1,0,2).numpy())\n",
    "        \n",
    "        transform_st2 = transforms.Compose([transforms.ToTensor(),transforms.RandomHorizontalFlip(0.35), transforms.Normalize([0.2860,],[0.3205,])]) \n",
    "        corr_images1_.append(transform_st2(img).permute(1,0,2).numpy())\n",
    "\n",
    "        transform_st3 = transforms.Compose([transforms.ToTensor(),transforms.RandomRotation(angle), transforms.RandomVerticalFlip(0.60),\n",
    "                                    transforms.RandomAffine(angle),  transforms.Normalize([0.2860,],[0.3205,])]) \n",
    "        corr_images1_.append(transform_st3(img).permute(1,0,2).numpy())\n",
    "\n",
    "\n",
    "        transform_st4 = transforms.Compose([transforms.ToTensor(), transforms.RandomVerticalFlip(0.25),\n",
    "                                        transforms.RandomAffine(angle), transforms.Normalize([0.2860,],[0.3205,])]) \n",
    "        corr_images1_.append(transform_st4(img).permute(1,0,2).numpy())\n",
    "\n",
    "        transform_st5 = transforms.Compose([transforms.ToTensor(), transforms.RandomVerticalFlip(0.45),transforms.Normalize([0.2860,],[0.3205,])]) \n",
    "        corr_images1_.append(transform_st5(img).permute(1,0,2).numpy())\n",
    "\n",
    "        transform_st6 = transforms.Compose([transforms.ToTensor(), transforms.RandomRotation(angle), \\\n",
    "                                              transforms.RandomVerticalFlip(0.75),transforms.Normalize([0.2860,],[0.3205,])]) \n",
    "        corr_images1_.append(transform_st6(img).permute(1,0,2).numpy())\n",
    "\n",
    "        transform_st7 = transforms.Compose([transforms.ToTensor(),transforms.RandomHorizontalFlip(0.13), transforms.RandomAffine(angle),\\\n",
    "                                            transforms.Normalize([0.2860,],[0.3205,])]) \n",
    "        corr_images1_.append(transform_st7(img).permute(1,0,2).numpy())\n",
    "\n",
    "        transform_st8 = transforms.Compose([transforms.ToTensor(),transforms.RandomRotation(angle), transforms.RandomAffine(angle),\\\n",
    "                                            transforms.Normalize([0.2860,],[0.3205,])]) \n",
    "        corr_images1_.append(transform_st8(img).permute(1,0,2).numpy())\n",
    "\n",
    "\n",
    "        transform_st9 = transforms.Compose([transforms.ToTensor(),transforms.RandomRotation(angle), transforms.RandomHorizontalFlip(0.60),\n",
    "                                            transforms.RandomAffine(angle),transforms.Normalize([0.2860,],[0.3205,])]) \n",
    "        corr_images1_.append(transform_st9(img).permute(1,2,0).numpy())\n",
    "\n",
    "\n",
    "\n",
    "        # transform_st6 = transforms.Compose([transforms.ToTensor(),transforms.RandomRotation(angle), transforms.RandomVerticalFlip(0.75),\n",
    "        #                                     transforms.RandomAffine(angle)]) \n",
    "        # corr_images2_1.append(transform_st6(img).permute(1,2,0).numpy())\n",
    "\n",
    "        # transform_st7 = transforms.Compose([transforms.ToTensor(), transforms.RandomVerticalFlip(0.75),\n",
    "        #                                     transforms.RandomAffine(angle)]) \n",
    "        # corr_images2_1.append(transform_st7(img).permute(1,2,0).numpy())\n",
    "\n",
    "        # transform_st8 = transforms.Compose([transforms.ToTensor(), transforms.RandomVerticalFlip(0.75),]) \n",
    "        # corr_images2_1.append(transform_st8(img).permute(1,2,0).numpy())\n",
    "\n",
    "        # transform_st9 = transforms.Compose([transforms.ToTensor(), transforms.RandomRotation(angle), \\\n",
    "        #                                       transforms.RandomVerticalFlip(0.25),]) \n",
    "        # corr_images2_1.append(transform_st9(img).permute(1,2,0).numpy())\n",
    "\n",
    "\n",
    "\n",
    "corr_images_ = np.unique(corr_images1_, axis=0)\n",
    "idx = np.random.permutation(len(corr_images_))[:3000]\n",
    "corr_images_ = corr_images_[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "kniPq8x3lLMT"
   },
   "outputs": [],
   "source": [
    "corr_images3 = np.array(corr_images_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dDlGn8Z_lLT_",
    "outputId": "c0e3e487-a979-4db0-b43d-13b1781e4749"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3, 32, 32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normal dist + Structured + Random Pixel\n",
    "corr_images = np.concatenate([corr_images1, corr_images2, corr_images3])\n",
    "corr_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3zyNkvo5o2WB",
    "outputId": "0d4197c7-f640-48bf-dbdd-65f79c1fce59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique digits :  [0 1 2 3 4 5 6 8 9]\n",
      "\n",
      " counts :  [ 761  143 5253   44  103  100 3558    1   37]\n"
     ]
    }
   ],
   "source": [
    "corr_images = torch.tensor(corr_images).type(torch.FloatTensor).to('cuda')\n",
    "\n",
    "corr_label = []\n",
    "check_label = []\n",
    "\n",
    "model.eval()\n",
    "for img in corr_images:\n",
    "  img = img[None,].type('torch.cuda.FloatTensor')\n",
    "  with torch.no_grad():\n",
    "    pred = model(img).argmax()\n",
    "  corr_label.append(pred)\n",
    "  check_label.append(pred.item())\n",
    "\n",
    "digit,count = np.unique(check_label, return_counts=True)\n",
    "\n",
    "# Model's prediction on pseudorandom images\n",
    "print(\"\\nUnique digits : \",digit)\n",
    "\n",
    "print(\"\\n counts : \",count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "uG1i7FSwo2Y5"
   },
   "outputs": [],
   "source": [
    "pos_neg_layerwise = {}\n",
    "\n",
    "for i in layer_name:\n",
    "  b = layer_distinct_weights[i]\n",
    "\n",
    "  if len(b) == 3:\n",
    "    neg, zero, pos = b\n",
    "    pos_neg_layerwise[i] = {}\n",
    "    pos_neg_layerwise[i]['pos'] = pos\n",
    "    pos_neg_layerwise[i]['neg'] = neg\n",
    "    \n",
    "  elif len(b) == 2:\n",
    "    un1, un2 = b\n",
    "\n",
    "    if un2 == 0:\n",
    "      neg, zero = b\n",
    "      pos_neg_layerwise[i] = {}\n",
    "      pos_neg_layerwise[i]['neg'] = neg\n",
    "      \n",
    "    elif un1 == 0:\n",
    "      zero, pos = b\n",
    "      pos_neg_layerwise[i] = {}\n",
    "      pos_neg_layerwise[i]['pos'] = pos\n",
    "\n",
    "    else:\n",
    "      neg, pos = b\n",
    "      pos_neg_layerwise[i] = {}\n",
    "      pos_neg_layerwise[i]['pos'] = pos\n",
    "      pos_neg_layerwise[i]['neg'] = neg\n",
    "\n",
    "  else:\n",
    "     un = b[0] \n",
    "\n",
    "     if un > 0:\n",
    "       pos_neg_layerwise[i] = {}\n",
    "       pos_neg_layerwise[i]['pos'] = un\n",
    "\n",
    "     elif un < 0:\n",
    "       pos_neg_layerwise[i] = {}\n",
    "       pos_neg_layerwise[i]['neg'] = un\n",
    "\n",
    "     else:\n",
    "       pos_neg_layerwise[i] = {}\n",
    "       pos_neg_layerwise[i]['zero'] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JZ4zH-VAo2bH",
    "outputId": "d564ef73-c0a8-40ce-f5f6-aa3e02befbf0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base.0.weight': [-0.997970700263977, 0.0, 0.997970700263977],\n",
       " 'base.1.bias': [-1.002286434173584, 0.0, 1.0049375295639038],\n",
       " 'base.1.weight': [0.0, 1.0062828063964844],\n",
       " 'base.4.0.bn1.bias': [-1.0043219327926636, 0.0, 1.000535011291504],\n",
       " 'base.4.0.bn1.weight': [0.0, 1.0029528141021729],\n",
       " 'base.4.0.bn2.bias': [-1.0083696842193604, 0.0, 1.0035288333892822],\n",
       " 'base.4.0.bn2.weight': [0.0, 0.9989628195762634],\n",
       " 'base.4.0.conv1.weight': [-0.9992982745170593, 0.0, 0.9992982149124146],\n",
       " 'base.4.0.conv2.weight': [-0.9979877471923828, 0.0, 0.9979877471923828],\n",
       " 'base.4.1.bn1.bias': [-1.0035675764083862, 0.0, 1.0016995668411255],\n",
       " 'base.4.1.bn1.weight': [0.0, 1.001746654510498],\n",
       " 'base.4.1.bn2.bias': [-1.008662223815918, 0.0, 0.998275637626648],\n",
       " 'base.4.1.bn2.weight': [0.0, 0.9972245097160339],\n",
       " 'base.4.1.conv1.weight': [-0.9981896281242371, 0.0, 0.9981896281242371],\n",
       " 'base.4.1.conv2.weight': [-1.0003037452697754, 0.0, 1.0003037452697754],\n",
       " 'base.5.0.bn1.bias': [-1.0056942701339722, 0.0, 1.0005754232406616],\n",
       " 'base.5.0.bn1.weight': [0.0, 1.0037789344787598],\n",
       " 'base.5.0.bn2.bias': [-1.0034257173538208, 0.0, 1.002722144126892],\n",
       " 'base.5.0.bn2.weight': [0.0, 0.9999319314956665],\n",
       " 'base.5.0.conv1.weight': [-1.0036280155181885, 0.0, 1.0036280155181885],\n",
       " 'base.5.0.conv2.weight': [-1.0006707906723022, 0.0, 1.0006706714630127],\n",
       " 'base.5.0.downsample.0.weight': [-0.9982104897499084,\n",
       "  0.0,\n",
       "  0.9982104897499084],\n",
       " 'base.5.0.downsample.1.bias': [-1.0034257173538208, 0.0, 1.002722144126892],\n",
       " 'base.5.0.downsample.1.weight': [0.0, 1.0066616535186768],\n",
       " 'base.5.1.bn1.bias': [-1.0059788227081299, 0.0],\n",
       " 'base.5.1.bn1.weight': [0.0, 1.0059787034988403],\n",
       " 'base.5.1.bn2.bias': [-1.0099973678588867, 0.0, 1.0002497434616089],\n",
       " 'base.5.1.bn2.weight': [0.0, 0.9948470592498779],\n",
       " 'base.5.1.conv1.weight': [-1.000842809677124, 0.0, 1.0008429288864136],\n",
       " 'base.5.1.conv2.weight': [-0.9999493360519409, 0.0, 0.9999484419822693],\n",
       " 'base.6.0.bn1.bias': [-1.0127111673355103, 0.0, 0.993628203868866],\n",
       " 'base.6.0.bn1.weight': [0.0, 1.013394832611084],\n",
       " 'base.6.0.bn2.bias': [-1.0013892650604248, 0.0, 1.002809762954712],\n",
       " 'base.6.0.bn2.weight': [0.0, 1.0063414573669434],\n",
       " 'base.6.0.conv1.weight': [-0.9990434050559998, 0.0, 0.9990435838699341],\n",
       " 'base.6.0.conv2.weight': [-1.0041515827178955, 0.0, 1.0041072368621826],\n",
       " 'base.6.0.downsample.0.weight': [-0.9986178874969482,\n",
       "  0.0,\n",
       "  0.9986178874969482],\n",
       " 'base.6.0.downsample.1.bias': [-1.0013892650604248, 0.0, 1.002809762954712],\n",
       " 'base.6.0.downsample.1.weight': [-1.0046577453613281,\n",
       "  0.0,\n",
       "  0.9974761605262756],\n",
       " 'base.6.1.bn1.bias': [-1.0039376020431519, 0.0],\n",
       " 'base.6.1.bn1.weight': [0.0, 1.0038793087005615],\n",
       " 'base.6.1.bn2.bias': [-1.0051746368408203, 0.0, 1.0001323223114014],\n",
       " 'base.6.1.bn2.weight': [0.0, 0.997181236743927],\n",
       " 'base.6.1.conv1.weight': [-0.9996164441108704, 0.0, 0.9996131658554077],\n",
       " 'base.6.1.conv2.weight': [-1.0010912418365479, 0.0, 1.0010483264923096],\n",
       " 'base.7.0.bn1.bias': [-1.0132701396942139, 0.0, 1.0029336214065552],\n",
       " 'base.7.0.bn1.weight': [0.0, 1.0118685960769653],\n",
       " 'base.7.0.bn2.bias': [-0.9646565914154053, 0.0, 0.9782690405845642],\n",
       " 'base.7.0.bn2.weight': [0.0, 0.959900438785553],\n",
       " 'base.7.0.conv1.weight': [-0.9971515536308289, 0.0, 0.9971283078193665],\n",
       " 'base.7.0.conv2.weight': [-0.9794412851333618, 0.0, 0.9794382452964783],\n",
       " 'base.7.0.downsample.0.weight': [-0.9849801063537598,\n",
       "  0.0,\n",
       "  0.9849342107772827],\n",
       " 'base.7.0.downsample.1.bias': [-0.9646565914154053, 0.0, 0.9782690405845642],\n",
       " 'base.7.0.downsample.1.weight': [-0.9999861717224121,\n",
       "  0.0,\n",
       "  0.9638087749481201],\n",
       " 'base.7.1.bn1.bias': [-1.0207003355026245, 0.0, 0.9878734350204468],\n",
       " 'base.7.1.bn1.weight': [0.0, 1.021633267402649],\n",
       " 'base.7.1.bn2.bias': [-0.9805426001548767, 0.0, 0.9250654578208923],\n",
       " 'base.7.1.bn2.weight': [0.0, 0.9945676326751709],\n",
       " 'base.7.1.conv1.weight': [-1.0018998384475708, 0.0, 1.0018991231918335],\n",
       " 'base.7.1.conv2.weight': [-1.0150518417358398, 0.0, 1.014282464981079],\n",
       " 'final.bias': [-1.0000823736190796, 1.000348687171936],\n",
       " 'final.weight': [-1.0667845010757446, 0.0, 0.929437518119812]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_distinct_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "cCfKOm8no2du"
   },
   "outputs": [],
   "source": [
    "class PseudoData(Dataset):\n",
    "    \n",
    "    def __init__(self, data, label):\n",
    "        \n",
    "        self.data = data\n",
    "        self.label = label\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        return self.data[index], self.label[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "corr_label = torch.tensor(corr_label).type(torch.FloatTensor).to('cuda')\n",
    "pseudo_dataLoader = DataLoader(dataset = PseudoData(corr_images, corr_label), batch_size = 1000, shuffle=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "uAf424TSo2gc"
   },
   "outputs": [],
   "source": [
    "def getResult(test_example, test_label, model, st):\n",
    "    model.load_state_dict(st)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output_ = model(test_example.float()) \n",
    "        pred = output_.data.max(1, keepdim=True)[1]\n",
    "        z = pred.eq(test_label.data.view_as(pred)).flatten()\n",
    "        return torch.where(z == False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "qQkUUItGo2j2"
   },
   "outputs": [],
   "source": [
    "def populateResults(idx, batch_id, old_state, new_state, w_images, n_ic, n_oc, row, column, weight_name):\n",
    "  key = 'img_id'+str(idx)+'_batch_'+str(batch_id)\n",
    "  if key not in w_images:\n",
    "    w_images[key] = {}\n",
    "    w_images[key]['location'] = []\n",
    "    w_images[key]['location'].append((weight_name, n_ic, n_oc, row, column, old_state, new_state))\n",
    "    w_images[key]['weight_states'] = []\n",
    "    w_images[key]['weight_states'].append(weight_name + ' : ' + str(old_state) + ' --> ' + str(new_state))\n",
    "\n",
    "  else:\n",
    "    w_images[key]['location'].append((weight_name, n_ic, n_oc, row, column, old_state, new_state))\n",
    "    w_images[key]['weight_states'].append(weight_name + ' : ' + str(old_state) + ' --> ' + str(new_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "jCBF5EBvo2lz"
   },
   "outputs": [],
   "source": [
    "def populateResults1(idx, batch_id, old_state, new_state, w_images, row, column, weight_name):\n",
    "  key = 'img_id'+str(idx)+'_batch_'+str(batch_id)\n",
    "  if key not in w_images:\n",
    "    w_images[key] = {}\n",
    "    w_images[key]['location'] = []\n",
    "    w_images[key]['location'].append((weight_name, row, column, old_state, new_state))\n",
    "    w_images[key]['weight_states'] = []\n",
    "    w_images[key]['weight_states'].append(weight_name + ' : ' + str(old_state) + ' --> ' + str(new_state))\n",
    "\n",
    "  else:\n",
    "    w_images[key]['location'].append((weight_name, row, column, old_state, new_state))\n",
    "    w_images[key]['weight_states'].append(weight_name + ' : ' + str(old_state) + ' --> ' + str(new_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "Tb0jVhzUo2oX"
   },
   "outputs": [],
   "source": [
    "# For bias weight mutation\n",
    "\n",
    "def bias_mutation(st, superSet, w_images, img,label, batch_id, weight_name):\n",
    "\n",
    "  if ('pos' in pos_neg_layerwise[weight_name]) and ('neg' in pos_neg_layerwise[weight_name]):\n",
    "    un = pos_neg_layerwise[weight_name]['pos'], pos_neg_layerwise[weight_name]['neg']\n",
    "\n",
    "    var_neg1 = torch.where(st==un[1])[0]\n",
    "    var_1 = torch.where(st==un[0])[0]\n",
    "\n",
    "  else:\n",
    "    try:\n",
    "      un = pos_neg_layerwise[weight_name]['pos']\n",
    "\n",
    "      var_neg1 = torch.where(st==un*1000)[0]\n",
    "      var_1 = torch.where(st==un)[0]\n",
    "\n",
    "    except KeyError:\n",
    "      un = pos_neg_layerwise[weight_name]['neg']\n",
    "\n",
    "      var_neg1 = torch.where(st==un)[0]\n",
    "      var_1 = torch.where(st==un*1000)[0]\n",
    "\n",
    "\n",
    "  if (var_neg1.nelement() == 0.) and (var_1.nelement() == 0.):\n",
    "    pass\n",
    "  else:\n",
    "    for column in torch.cat((var_neg1, var_1), 0):\n",
    "      row = 0\n",
    "      column = column.item()\n",
    "\n",
    "      org_val_real = st[column].item() # For bias part\n",
    "\n",
    "      if type(un) == tuple:\n",
    "          pos, neg = un   # Positive, Negative \n",
    "          if (org_val_real < 0.) and (org_val_real == neg):\n",
    "            org_val = neg\n",
    "\n",
    "            old_state = neg\n",
    "            new_state = 0.\n",
    "            superSet.add((weight_name, row, column, -1, 0))\n",
    "\n",
    "            st[column] = new_state\n",
    "\n",
    "            \"\"\"getResult() will tell whether the passed test example is classified to other class\n",
    "            or not after mutation, if it is classified to other class, then it returns True else False and \n",
    "            populateResults() will maintain the record of test examples which has been misclassified\n",
    "            like location(row(which neuron), column(indices)), old_state(old_value of weight element), \n",
    "            new_state (mutated value of weight element), images\"\"\"\n",
    "\n",
    "            \n",
    "            [populateResults1(idx.item(), batch_id, -1, 0, w_images, row, column, weight_name) \\\n",
    "             for idx in getResult(img, label, model, state_dict)]\n",
    "          \n",
    "            new_state = pos\n",
    "            superSet.add((weight_name, row, column, -1, 1))\n",
    "\n",
    "            st[column] = new_state\n",
    "\n",
    "            [populateResults1(idx.item(), batch_id, -1, 1, w_images, row, column, weight_name) \\\n",
    "             for idx in getResult(img, label, model, state_dict)]\n",
    "            \n",
    "            st[column] = org_val\n",
    "\n",
    "          elif (org_val_real > 0.) and (org_val_real == pos):\n",
    "\n",
    "            org_val = pos\n",
    "            old_state = pos\n",
    "            new_state = 0.\n",
    "            superSet.add((weight_name, row, column, 1, 0))\n",
    "            st[column] = new_state\n",
    "\n",
    "            [populateResults1(idx.item(), batch_id, 1, 0, w_images, row, column, weight_name) \\\n",
    "             for idx in getResult(img, label, model, state_dict)]\n",
    "          \n",
    "            new_state = neg\n",
    "            superSet.add((weight_name, row, column, 1, -1))\n",
    "\n",
    "            st[column] = new_state\n",
    "            \n",
    "            [populateResults1(idx.item(), batch_id, 1, -1, w_images, row, column, weight_name) \\\n",
    "             for idx in getResult(img, label, model, state_dict)]\n",
    "            \n",
    "            st[column] = org_val\n",
    "\n",
    "\n",
    "      \n",
    "      ############# Negative ################################\n",
    "      elif (un < 0) and (org_val_real == un):\n",
    "          org_val = un\n",
    "          old_state = un\n",
    "          new_state = 0.\n",
    "          superSet.add((weight_name, row, column, -1, 0))\n",
    "          st[column] = new_state\n",
    "            \n",
    "          [populateResults1(idx.item(), batch_id, -1, 0, w_images, row, column, weight_name) \\\n",
    "           for idx in getResult(img, label, model, state_dict)]\n",
    "          \n",
    "          st[column] = org_val\n",
    "\n",
    "\n",
    "      elif (un > 0) and (org_val_real == un):  ############# Positive  ################################\n",
    "          org_val = un\n",
    "          old_state = un\n",
    "          new_state = 0.\n",
    "          superSet.add((weight_name, row, column, 1, 0))\n",
    "          st[column] = new_state\n",
    "          [populateResults1(idx.item(), batch_id, 1, 0, w_images, row, column, weight_name) \\\n",
    "           for idx in getResult(img, label, model, state_dict)]\n",
    "\n",
    "          st[column] = org_val\n",
    "\n",
    "\n",
    "      #################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "oaZX0ddzo2rG"
   },
   "outputs": [],
   "source": [
    "# Saving length of shape for each layer\n",
    "layers_shape = {}\n",
    "\n",
    "for i in layer_name:\n",
    "  layers_shape[i] = len(state_dict[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PFbz-pOzo2tw",
    "outputId": "c136bd0a-ded9-4394-c566-319860b20485"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'base.0.weight': 4, 'base.1.bias': 1, 'base.1.weight': 1, 'base.4.0.conv1.weight': 4, 'base.4.0.bn1.bias': 1, 'base.4.0.bn1.weight': 1, 'base.4.0.conv2.weight': 4, 'base.4.0.bn2.bias': 1, 'base.4.0.bn2.weight': 1, 'base.4.1.conv1.weight': 4, 'base.4.1.bn1.bias': 1, 'base.4.1.bn1.weight': 1, 'base.4.1.conv2.weight': 4, 'base.4.1.bn2.bias': 1, 'base.4.1.bn2.weight': 1, 'base.5.0.conv1.weight': 4, 'base.5.0.bn1.bias': 1, 'base.5.0.bn1.weight': 1, 'base.5.0.conv2.weight': 4, 'base.5.0.bn2.bias': 1, 'base.5.0.bn2.weight': 1, 'base.5.0.downsample.0.weight': 4, 'base.5.0.downsample.1.bias': 1, 'base.5.0.downsample.1.weight': 1, 'base.5.1.conv1.weight': 4, 'base.5.1.bn1.bias': 1, 'base.5.1.bn1.weight': 1, 'base.5.1.conv2.weight': 4, 'base.5.1.bn2.bias': 1, 'base.5.1.bn2.weight': 1, 'base.6.0.conv1.weight': 4, 'base.6.0.bn1.bias': 1, 'base.6.0.bn1.weight': 1, 'base.6.0.conv2.weight': 4, 'base.6.0.bn2.bias': 1, 'base.6.0.bn2.weight': 1, 'base.6.0.downsample.0.weight': 4, 'base.6.0.downsample.1.bias': 1, 'base.6.0.downsample.1.weight': 1, 'base.6.1.conv1.weight': 4, 'base.6.1.bn1.bias': 1, 'base.6.1.bn1.weight': 1, 'base.6.1.conv2.weight': 4, 'base.6.1.bn2.bias': 1, 'base.6.1.bn2.weight': 1, 'base.7.0.conv1.weight': 4, 'base.7.0.bn1.bias': 1, 'base.7.0.bn1.weight': 1, 'base.7.0.conv2.weight': 4, 'base.7.0.bn2.bias': 1, 'base.7.0.bn2.weight': 1, 'base.7.0.downsample.0.weight': 4, 'base.7.0.downsample.1.bias': 1, 'base.7.0.downsample.1.weight': 1, 'base.7.1.conv1.weight': 4, 'base.7.1.bn1.bias': 1, 'base.7.1.bn1.weight': 1, 'base.7.1.conv2.weight': 4, 'base.7.1.bn2.bias': 1, 'base.7.1.bn2.weight': 1, 'final.bias': 1, 'final.weight': 2}\n"
     ]
    }
   ],
   "source": [
    "print(layers_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NuNNlCH0o2wG",
    "outputId": "59bf24d0-46a6-4beb-a5f2-15d9ca1a53d5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of elements in superSet is : 84447. \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|         | 1/10 [44:44<6:42:42, 2684.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 46345 locations that did not match.\n",
      "\n",
      "Total number of locations 38102 are encompassed out of 84447.\n",
      "Total number of elements in a weight matrix is  84447\n",
      "Total number of images fetched  53 \n",
      "\n",
      "Output \n",
      "\n",
      "[(53, 38102, 84447)]\n",
      "Total number of elements in superSet is : 84447. \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|        | 2/10 [1:29:34<5:58:21, 2687.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 44581 locations that did not match.\n",
      "\n",
      "Total number of locations 39866 are encompassed out of 84447.\n",
      "Total number of elements in a weight matrix is  84447\n",
      "Total number of images fetched  75 \n",
      "\n",
      "Output \n",
      "\n",
      "[(53, 38102, 84447), (75, 39866, 84447)]\n",
      "Total number of elements in superSet is : 84447. \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|       | 3/10 [2:14:27<5:13:52, 2690.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 43583 locations that did not match.\n",
      "\n",
      "Total number of locations 40864 are encompassed out of 84447.\n",
      "Total number of elements in a weight matrix is  84447\n",
      "Total number of images fetched  101 \n",
      "\n",
      "Output \n",
      "\n",
      "[(53, 38102, 84447), (75, 39866, 84447), (101, 40864, 84447)]\n",
      "Total number of elements in superSet is : 84447. \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|      | 4/10 [2:59:29<4:29:28, 2694.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 42835 locations that did not match.\n",
      "\n",
      "Total number of locations 41612 are encompassed out of 84447.\n",
      "Total number of elements in a weight matrix is  84447\n",
      "Total number of images fetched  118 \n",
      "\n",
      "Output \n",
      "\n",
      "[(53, 38102, 84447), (75, 39866, 84447), (101, 40864, 84447), (118, 41612, 84447)]\n",
      "Total number of elements in superSet is : 84447. \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|     | 5/10 [3:44:23<3:44:31, 2694.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 30292 locations that did not match.\n",
      "\n",
      "Total number of locations 54155 are encompassed out of 84447.\n",
      "Total number of elements in a weight matrix is  84447\n",
      "Total number of images fetched  165 \n",
      "\n",
      "Output \n",
      "\n",
      "[(53, 38102, 84447), (75, 39866, 84447), (101, 40864, 84447), (118, 41612, 84447), (165, 54155, 84447)]\n",
      "Total number of elements in superSet is : 84447. \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|    | 6/10 [4:29:18<2:59:39, 2694.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 25136 locations that did not match.\n",
      "\n",
      "Total number of locations 59311 are encompassed out of 84447.\n",
      "Total number of elements in a weight matrix is  84447\n",
      "Total number of images fetched  186 \n",
      "\n",
      "Output \n",
      "\n",
      "[(53, 38102, 84447), (75, 39866, 84447), (101, 40864, 84447), (118, 41612, 84447), (165, 54155, 84447), (186, 59311, 84447)]\n",
      "Total number of elements in superSet is : 84447. \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|   | 7/10 [5:14:24<2:14:55, 2698.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 23907 locations that did not match.\n",
      "\n",
      "Total number of locations 60540 are encompassed out of 84447.\n",
      "Total number of elements in a weight matrix is  84447\n",
      "Total number of images fetched  210 \n",
      "\n",
      "Output \n",
      "\n",
      "[(53, 38102, 84447), (75, 39866, 84447), (101, 40864, 84447), (118, 41612, 84447), (165, 54155, 84447), (186, 59311, 84447), (210, 60540, 84447)]\n",
      "Total number of elements in superSet is : 84447. \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|  | 8/10 [5:59:39<1:30:07, 2703.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 21990 locations that did not match.\n",
      "\n",
      "Total number of locations 62457 are encompassed out of 84447.\n",
      "Total number of elements in a weight matrix is  84447\n",
      "Total number of images fetched  212 \n",
      "\n",
      "Output \n",
      "\n",
      "[(53, 38102, 84447), (75, 39866, 84447), (101, 40864, 84447), (118, 41612, 84447), (165, 54155, 84447), (186, 59311, 84447), (210, 60540, 84447), (212, 62457, 84447)]\n",
      "Total number of elements in superSet is : 84447. \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%| | 9/10 [6:45:03<45:09, 2709.90s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 21389 locations that did not match.\n",
      "\n",
      "Total number of locations 63058 are encompassed out of 84447.\n",
      "Total number of elements in a weight matrix is  84447\n",
      "Total number of images fetched  220 \n",
      "\n",
      "Output \n",
      "\n",
      "[(53, 38102, 84447), (75, 39866, 84447), (101, 40864, 84447), (118, 41612, 84447), (165, 54155, 84447), (186, 59311, 84447), (210, 60540, 84447), (212, 62457, 84447), (220, 63058, 84447)]\n",
      "Total number of elements in superSet is : 84447. \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 10/10 [7:30:33<00:00, 2703.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 20965 locations that did not match.\n",
      "\n",
      "Total number of locations 63482 are encompassed out of 84447.\n",
      "Total number of elements in a weight matrix is  84447\n",
      "Total number of images fetched  221 \n",
      "\n",
      "Output \n",
      "\n",
      "[(53, 38102, 84447), (75, 39866, 84447), (101, 40864, 84447), (118, 41612, 84447), (165, 54155, 84447), (186, 59311, 84447), (210, 60540, 84447), (212, 62457, 84447), (220, 63058, 84447), (221, 63482, 84447)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Injecting fault by mutating the ternary weight and check whether it is classified to other class than before by the model. \n",
    "\n",
    "fetchimg_totImg_coverage_totCov = []\n",
    "\n",
    "w_images = {}\n",
    "superSet = set()\n",
    "\n",
    "\n",
    "batch_id = 0\n",
    "for img,label  in tqdm(pseudo_dataLoader):\n",
    "\n",
    "    for weight_name in layer_name:\n",
    "\n",
    "        # Selecting hidden layer to iterate its weight elements\n",
    "        st = state_dict[weight_name]\n",
    "\n",
    "\n",
    "        if ('bias' in weight_name) or (layers_shape[weight_name] == 1):\n",
    "          bias_mutation(st, superSet, w_images, img,label, batch_id, weight_name)\n",
    "          continue\n",
    "\n",
    "        if ('conv' in weight_name) or (layers_shape[weight_name] == 4):\n",
    "\n",
    "          ic, oc, k,k1 = st.shape # ic = in_channel, oc= out_channel, k and k1 is the number of rows and columns of a kernel\n",
    "\n",
    "          for n_ic in range(ic):\n",
    "            for n_oc in range(oc):\n",
    "              kt =  st[n_ic][n_oc] # Fetching Kernel matrix or tensor\n",
    "\n",
    "              # Iterating number of neurons for the selected hidden layer\n",
    "              for row in range(k):\n",
    "\n",
    "                # Fetching indices for -w and w weight value for the selected kernel\n",
    "                \"\"\"Since each kernel has weight vector, so by fetching indices\n",
    "                we will come to know, which indices of the kernel contains weights -w or w weight\"\"\"\n",
    "\n",
    "                ###########################################################################################\n",
    "\n",
    "                if ('pos' in pos_neg_layerwise[weight_name]) and ('neg' in pos_neg_layerwise[weight_name]):\n",
    "                  un = pos_neg_layerwise[weight_name]['pos'], pos_neg_layerwise[weight_name]['neg']\n",
    "\n",
    "                  var_neg1 = torch.where(kt[row]==un[1])[0]\n",
    "                  var_1 = torch.where(kt[row]==un[0])[0]\n",
    "\n",
    "                else:\n",
    "                  try:\n",
    "                    un = pos_neg_layerwise[weight_name]['pos']\n",
    "\n",
    "                    var_neg1 = torch.where(kt[row]==un*1000)[0]\n",
    "                    var_1 = torch.where(kt[row]==un)[0]\n",
    "\n",
    "                  except KeyError:\n",
    "                    un = pos_neg_layerwise[weight_name]['neg']\n",
    "\n",
    "                    var_neg1 = torch.where(kt[row]==un)[0]\n",
    "                    var_1 = torch.where(kt[row]==un*1000)[0]\n",
    "\n",
    "                ##########################################################################################\n",
    "\n",
    "                if (var_neg1.nelement() == 0.) and (var_1.nelement() == 0.):\n",
    "                  continue\n",
    "                else:\n",
    "                  for column in torch.cat((var_neg1, var_1), 0):\n",
    "\n",
    "                    column = column.item()\n",
    "\n",
    "                    org_val_real = kt[row][column].item()\n",
    "\n",
    "                    ##################################################################################################################\n",
    "\n",
    "                    if type(un) == tuple:\n",
    "                      pos, neg = un  # Positive, Negative\n",
    "\n",
    "                      if (org_val_real < 0.) and (org_val_real == neg):\n",
    "                        org_val = neg\n",
    "                        old_state = neg\n",
    "                        new_state = 0.\n",
    "                        superSet.add((weight_name, n_ic, n_oc, row, column, -1, 0))\n",
    "\n",
    "                        st[n_ic][n_oc][row][column] = new_state\n",
    "                        \n",
    "                        \"\"\"getResult() will tell whether the passed test example is classified to other class\n",
    "                        or not after mutation, if it is classified to other class, then it returns True else False and \n",
    "                        populateResults() will maintain the record of test examples which has been misclassified\n",
    "                        like location(row(which neuron), column(indices)), old_state(old_value of weight element), \n",
    "                        new_state (mutated value of weight element), images\"\"\"\n",
    "\n",
    "                        \n",
    "                        [populateResults(idx.item(), batch_id, -1, 0, w_images, n_ic, n_oc, row, column, weight_name) \\\n",
    "                         for idx in getResult(img, label, model, state_dict)]\n",
    "                        \n",
    "                        new_state = pos\n",
    "                        superSet.add((weight_name, n_ic, n_oc, row, column, -1, 1))\n",
    "                        st[n_ic][n_oc][row][column] = new_state\n",
    "\n",
    "                            \n",
    "                        [populateResults(idx.item(), batch_id, -1, 1, w_images, n_ic, n_oc, row, column, weight_name) \\\n",
    "                         for idx in getResult(img, label, model, state_dict)]\n",
    "                        st[n_ic][n_oc][row][column] = org_val # Setting back to original value\n",
    "\n",
    "                    \n",
    "                      elif (org_val_real > 0.) and (org_val_real == pos):\n",
    "                        org_val = pos\n",
    "                        old_state = pos\n",
    "                        new_state = 0.\n",
    "                        superSet.add((weight_name, n_ic, n_oc, row, column, 1, 0))\n",
    "\n",
    "                        st[n_ic][n_oc][row][column] = new_state\n",
    "                        \n",
    "                        [populateResults(idx.item(), batch_id, 1, 0, w_images, n_ic, n_oc, row, column, weight_name) \\\n",
    "                         for idx in getResult(img, label, model, state_dict)]\n",
    "                        \n",
    "                        new_state = neg\n",
    "                        superSet.add((weight_name, n_ic, n_oc, row, column, 1, -1))\n",
    "                        st[n_ic][n_oc][row][column] = new_state\n",
    "\n",
    "                            \n",
    "                        [populateResults(idx.item(), batch_id, 1, -1, w_images, n_ic, n_oc, row, column, weight_name) \\\n",
    "                         for idx in getResult(img, label, model, state_dict)]\n",
    "                        \n",
    "                        st[n_ic][n_oc][row][column] = org_val # Setting back to original value\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    elif (un < 0) and (org_val_real == un): ########################## Negative ###########################################\n",
    "                      org_val = un\n",
    "                      old_state = un\n",
    "                      new_state = 0.\n",
    "                      superSet.add((weight_name,  n_ic, n_oc, row, column, -1, 0))\n",
    "                      st[n_ic][n_oc][row][column] = new_state\n",
    "                        \n",
    "                      [populateResults(idx.item(), batch_id, -1, 0, w_images, n_ic, n_oc, row, column, weight_name) \\\n",
    "                       for idx in getResult(img, label, model, state_dict)]\n",
    "                      \n",
    "                      st[n_ic][n_oc][row][column] = org_val\n",
    "\n",
    "\n",
    "                    elif (un > 0) and (org_val_real == un): ########################## Positive ###########################################\n",
    "                      org_val = un\n",
    "                      old_state = un\n",
    "                      new_state = 0.\n",
    "                      superSet.add((weight_name, n_ic, n_oc, row, column, 1, 0))\n",
    "                      st[n_ic][n_oc][row][column] = new_state\n",
    "                        \n",
    "                      [populateResults(idx.item(), batch_id, 1, 0, w_images, n_ic, n_oc, row, column, weight_name) \\\n",
    "                       for idx in getResult(img, label, model, state_dict)]\n",
    "                      st[n_ic][n_oc][row][column] = org_val\n",
    "\n",
    "\n",
    "    \n",
    "            \n",
    "        \n",
    "        # For fully connected layers\n",
    "        elif layers_shape[weight_name] == 2:\n",
    "          rows = state_dict[weight_name].shape[0]\n",
    "\n",
    "          # Iterating number of neurons for the selected hidden layer\n",
    "          for row in range(rows):\n",
    "\n",
    "            # Fetching indices for -w and w weight value for the selected neuron\n",
    "            \"\"\"Since each neuron has weight vector, so by fetching indices\n",
    "            we will come to know, which indices of the neuron contains weights -w or w weight\"\"\"\n",
    "\n",
    "            #########################################################################################\n",
    "\n",
    "            if ('pos' in pos_neg_layerwise[weight_name]) and ('neg' in pos_neg_layerwise[weight_name]):\n",
    "              un = pos_neg_layerwise[weight_name]['pos'], pos_neg_layerwise[weight_name]['neg']\n",
    "\n",
    "              var_neg1 = torch.where(st[row]==un[1])[0]\n",
    "              var_1 = torch.where(st[row]==un[0])[0]\n",
    "\n",
    "            else:\n",
    "              try:\n",
    "                un = pos_neg_layerwise[weight_name]['pos']\n",
    "\n",
    "                var_neg1 = torch.where(st[row]==un*1000)[0]\n",
    "                var_1 = torch.where(st[row]==un)[0]\n",
    "\n",
    "              except KeyError:\n",
    "                un = pos_neg_layerwise[weight_name]['neg']\n",
    "\n",
    "                var_neg1 = torch.where(st[row]==un)[0]\n",
    "                var_1 = torch.where(st[row]==un*1000)[0]\n",
    "\n",
    "\n",
    "            #########################################################################################\n",
    "\n",
    "\n",
    "            if (var_neg1.nelement() == 0.) and (var_1.nelement() == 0.):\n",
    "              continue\n",
    "            else:\n",
    "              for column in torch.cat((var_neg1, var_1), 0):\n",
    "\n",
    "                column = column.item()\n",
    "\n",
    "                org_val_v1 = st[row][column].item()\n",
    "\n",
    "\n",
    "                #########################################################################################\n",
    "\n",
    "                if type(un) == tuple:\n",
    "                  pos, neg = un  # Positive, Negative\n",
    "\n",
    "                  if (org_val_v1 < 0.) and (neg == org_val_v1):\n",
    "\n",
    "                    org_val = neg\n",
    "                    old_state = neg\n",
    "                    new_state = 0.\n",
    "                    superSet.add((weight_name, row, column, -1, 0))\n",
    "\n",
    "                    st[row][column] = new_state\n",
    "                    \n",
    "                    [populateResults1(idx.item(), batch_id, -1, 0, w_images, row, column, weight_name) \\\n",
    "                     for idx in getResult(img, label, model, state_dict)]\n",
    "                    \n",
    "                    new_state = pos\n",
    "                    superSet.add((weight_name, row, column, -1, 1))\n",
    "                    st[row][column] = new_state\n",
    "                        \n",
    "                    [populateResults1(idx.item(), batch_id, -1, 1, w_images, row, column, weight_name) \\\n",
    "                     for idx in getResult(img, label, model, state_dict)]\n",
    "                    st[row][column] = org_val\n",
    "\n",
    "\n",
    "                  elif (org_val_v1 > 0.) and (pos == org_val_v1):\n",
    "\n",
    "                    org_val = pos\n",
    "                    old_state = pos\n",
    "                    new_state = 0.\n",
    "                    superSet.add((weight_name, row, column, 1, 0))\n",
    "\n",
    "                    st[row][column] = new_state\n",
    "                    \n",
    "                    [populateResults1(idx.item(), batch_id, 1, 0, w_images, row, column, weight_name) \\\n",
    "                     for idx in getResult(img, label, model, state_dict)]\n",
    "                    \n",
    "                    new_state = neg\n",
    "                    superSet.add((weight_name, row, column, 1, -1))\n",
    "                    st[row][column] = new_state\n",
    "                        \n",
    "                    [populateResults1(idx.item(), batch_id, 1, -1, w_images, row, column, weight_name) \\\n",
    "                     for idx in getResult(img, label, model, state_dict)]\n",
    "                    st[row][column] = org_val\n",
    "\n",
    "\n",
    " \n",
    "                elif (un < 0) and (un == org_val_v1):  ################# Negative ###############################################################\n",
    "                  org_val = un  \n",
    "                  old_state = un\n",
    "                  new_state = 0.\n",
    "                  superSet.add((weight_name, row, column, -1, 0))\n",
    "                  st[row][column] = new_state\n",
    "                    \n",
    "                  [populateResults1(idx.item(), batch_id, -1, 0, w_images, row, column, weight_name) \\\n",
    "                   for idx in getResult(img, label, model, state_dict)]\n",
    "\n",
    "                  st[row][column] = org_val\n",
    "\n",
    "\n",
    "                elif (un > 0) and (un == org_val_v1):  ################# Positive ###############################################################\n",
    "                  org_val = un\n",
    "                  old_state = un\n",
    "                  new_state = 0.\n",
    "                  superSet.add((weight_name, row, column, 1, 0))\n",
    "                  st[row][column] = new_state\n",
    "                    \n",
    "                  [populateResults1(idx.item(), batch_id, 1, 0, w_images, row, column, weight_name) \\\n",
    "                   for idx in getResult(img, label, model, state_dict)]\n",
    "                  \n",
    "                  st[row][column] = org_val\n",
    "\n",
    "\n",
    "    batch_id += 1\n",
    "\n",
    "\n",
    "    # Fetching locations and total number of images\n",
    "    ################################################\n",
    "\n",
    "    print(f\"Total number of elements in superSet is : {len(superSet)}.\",'\\n')\n",
    "\n",
    "    # joblib.dump(w_images, open('w_images_'+str(batch_id)+'.pkl','wb')) \n",
    "\n",
    "    image_locCounts = [] # (image_id,locationCounts)\n",
    "\n",
    "    for img in w_images.keys():\n",
    "        locCounts = len(w_images[img]['location'])\n",
    "        image_locCounts.append((locCounts,img))\n",
    "            \n",
    "    image_locCounts.sort(reverse=True)\n",
    "\n",
    "\n",
    "    netSetOfImages = []\n",
    "    progressingSet = []\n",
    "    max_locCounts_img = image_locCounts[0][1] # An image that has encompassed maximum locations.\n",
    "\n",
    "    netSetOfImages.append(max_locCounts_img)\n",
    "    progressingSet += set(w_images[max_locCounts_img]['location'])\n",
    "\n",
    "\n",
    "    for i in range(1,len(image_locCounts)):\n",
    "        img = image_locCounts[i][1]\n",
    "        locs = w_images[img]['location']\n",
    "\n",
    "        if set(locs).issubset(progressingSet):\n",
    "            pass\n",
    "        else:\n",
    "            progressingSet += list(set(locs) - set(progressingSet))\n",
    "            netSetOfImages.append(img)\n",
    "\n",
    "\n",
    "    if set(superSet).issubset(progressingSet):\n",
    "        print(\"All match done!\")\n",
    "    else:\n",
    "        print(\"There are {} locations that did not match.\".format(len(set(superSet) - set(progressingSet))))\n",
    "\n",
    "    print()\n",
    "    print(\"Total number of locations {} are encompassed out of {}.\".format(len(set(progressingSet)), len(superSet)))\n",
    "\n",
    "\n",
    "    print(\"Total number of elements in a weight matrix is \", len(superSet))\n",
    "\n",
    "    print(\"Total number of images fetched \",len(netSetOfImages),'\\n')\n",
    "\n",
    "    fetchimg_totImg_coverage_totCov.append(( len(netSetOfImages), len(set(progressingSet)), len(superSet) ))\n",
    "\n",
    "    # joblib.dump(fetchimg_totImg_coverage_totCov, open('res'+str(numImgs)+'.pkl','wb')) \n",
    "\n",
    "    # Superset \n",
    "    # joblib.dump(superSet, open('superSet.pkl','wb'))    \n",
    "\n",
    "                \n",
    "      \n",
    "    print(\"Output \\n\")\n",
    "    print(fetchimg_totImg_coverage_totCov)\n",
    "\n",
    "    #joblib.dump(fetchimg_totImg_coverage_totCov, open('op_128.pkl', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TO_jTLFQl65o",
    "outputId": "924ba7a4-2c2c-4754-87c5-d2471a23e8cf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7517377763567682"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fault coverage \n",
    "fetchimg_totImg_coverage_totCov[-1][1]/fetchimg_totImg_coverage_totCov[-1][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pqdhYfIul8n3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Main_ResNet-18_Sequence.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1e27c11f831443e987cc77068fca71c4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_abd74acda0794e199ab77a4807078112",
       "IPY_MODEL_63f4a074909f40159f9cdfed7776a8b5",
       "IPY_MODEL_e46cb067c2874e199328a6e7053c44ad"
      ],
      "layout": "IPY_MODEL_8bf5a2f9c6af47fc99794231d85f6be6"
     }
    },
    "22f063e483974dc0963873ee184c6d5c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4cb5c328c5e7439ca898fcc8219c0288": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "63f4a074909f40159f9cdfed7776a8b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c115fd574064461eaa866428253b58d0",
      "max": 46830571,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9a93212993fa4d11aafb6a7a62827a77",
      "value": 46830571
     }
    },
    "6d5532489ef74deeb7422c57867859b0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8bf5a2f9c6af47fc99794231d85f6be6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9a93212993fa4d11aafb6a7a62827a77": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "abd74acda0794e199ab77a4807078112": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c31df19d7d134a0393d202497be264ea",
      "placeholder": "",
      "style": "IPY_MODEL_6d5532489ef74deeb7422c57867859b0",
      "value": "100%"
     }
    },
    "c115fd574064461eaa866428253b58d0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c31df19d7d134a0393d202497be264ea": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e46cb067c2874e199328a6e7053c44ad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4cb5c328c5e7439ca898fcc8219c0288",
      "placeholder": "",
      "style": "IPY_MODEL_22f063e483974dc0963873ee184c6d5c",
      "value": " 44.7M/44.7M [00:02&lt;00:00, 19.1MB/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
