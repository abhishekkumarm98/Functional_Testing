{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd MNIST/AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "7GjZ6db0s6sS"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys, os, random\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle, gzip\n",
    "from tqdm import tqdm,tqdm_notebook\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "nsdPOAoGs516"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "seed_num = 81\n",
    "\n",
    "# For reproducibility when you run the file with .py\n",
    "torch.cuda.is_available()\n",
    "torch.manual_seed(seed_num)\n",
    "torch.cuda.manual_seed(seed_num)\n",
    "np.random.seed(seed_num)\n",
    "random.seed(seed_num)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "torch.backends.cudnn.deterministic =True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "OU064Mmps517"
   },
   "outputs": [],
   "source": [
    "# Data Augmentation \n",
    "train_transform = transforms.Compose([transforms.RandomRotation(35), transforms.RandomHorizontalFlip(),transforms.ToTensor(),transforms.Normalize([0.1307,],[0.3081,])])\n",
    "test_transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize([0.1307,],[0.3081,])])\n",
    "\n",
    "# Splitting the training and test datasets\n",
    "train_data = datasets.MNIST(os.getcwd(), train=True,\n",
    "                              download=True, transform=train_transform)\n",
    "test_data = datasets.MNIST(os.getcwd(), train=False,\n",
    "                             download=True, transform=test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "u2Hj49-n_Ks9"
   },
   "outputs": [],
   "source": [
    "# Split the training set indices into training and validation set indices using 84:16 ratio\n",
    "np.random.seed(seed_num)\n",
    "len_trainset = len(train_data)\n",
    "index_list = list(range(len_trainset))\n",
    "np.random.shuffle(index_list)\n",
    "split_index = 50000\n",
    "train_indices, valid_indices =  index_list[:split_index], index_list[split_index:]\n",
    "\n",
    "# Creating Samplers for training and validation set using the indices\n",
    "np.random.seed(seed_num)\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(valid_indices)\n",
    "\n",
    "torch.manual_seed(seed_num)\n",
    "\n",
    "train_iterator = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\n",
    "val_iterator = DataLoader(train_data, batch_size=batch_size, sampler=valid_sampler)\n",
    "test_iterator = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ZefivYB7-SV7"
   },
   "outputs": [],
   "source": [
    "# AlexNet Model\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, 1), #in_channels, out_channels, kernel_size, stride, padding\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Conv2d(32, 64, 3, padding = 1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace = True),\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, padding = 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Conv2d(128, 256, 3, padding = 1),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Conv2d(256, 256, 3, padding = 1),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256 * 6 * 6, 256),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Linear(256,256),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Linear(256, output_dim),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        h = x.view(x.size(0), -1)\n",
    "        x = self.classifier(h)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hXXfGnBe-SWA",
    "outputId": "24f62414-bdfd-4b85-fa27-d3713a2ab3f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:\n",
      " AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (10): ReLU(inplace=True)\n",
      "    (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.1, inplace=False)\n",
      "    (1): Linear(in_features=9216, out_features=256, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Linear(in_features=256, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(seed_num)\n",
    "unit=128\n",
    "\n",
    "# Summary\n",
    "model = AlexNet(10)\n",
    "print(\"Model:\\n\",model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EDONLqRh-SWE",
    "outputId": "38ef4d08-6002-4760-db82-5c7efb905d2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['features.0.weight', 'features.0.bias', 'features.1.weight', 'features.1.bias', 'features.3.weight', 'features.3.bias', 'features.6.weight', 'features.6.bias', 'features.7.weight', 'features.7.bias', 'features.9.weight', 'features.9.bias', 'features.11.weight', 'features.11.bias', 'classifier.1.weight', 'classifier.1.bias', 'classifier.3.weight', 'classifier.3.bias', 'classifier.5.weight', 'classifier.5.bias']\n"
     ]
    }
   ],
   "source": [
    "# Layer names\n",
    "layer_name = [n for n, p in model.named_parameters()]\n",
    "print(layer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7XzsdBoU_nCo",
    "outputId": "e89f8a54-11c6-42cc-bb20-eee76eaf26f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights done !\n",
      "features.0.weight tensor(135, device='cuda:0')\n",
      "features.0.bias tensor(22, device='cuda:0')\n",
      "features.1.weight tensor(24, device='cuda:0')\n",
      "features.1.bias tensor(13, device='cuda:0')\n",
      "features.3.weight tensor(452, device='cuda:0')\n",
      "features.3.bias tensor(32, device='cuda:0')\n",
      "features.6.weight tensor(492, device='cuda:0')\n",
      "features.6.bias tensor(35, device='cuda:0')\n",
      "features.7.weight tensor(94, device='cuda:0')\n",
      "features.7.bias tensor(69, device='cuda:0')\n",
      "features.9.weight tensor(502, device='cuda:0')\n",
      "features.9.bias tensor(26, device='cuda:0')\n",
      "features.11.weight tensor(366, device='cuda:0')\n",
      "features.11.bias tensor(19, device='cuda:0')\n",
      "classifier.1.weight tensor(1559, device='cuda:0')\n",
      "classifier.1.bias tensor(36, device='cuda:0')\n",
      "classifier.3.weight tensor(287, device='cuda:0')\n",
      "classifier.3.bias tensor(47, device='cuda:0')\n",
      "classifier.5.weight tensor(222, device='cuda:0')\n",
      "classifier.5.bias tensor(6, device='cuda:0')\n",
      "Total Parameters: tensor(4438, device='cuda:0') \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading the weights of ternary model \n",
    "model = torch.load('AlexNet_mnist_Quant.pt')\n",
    "model = model.cuda()\n",
    "print(\"Loading weights done !\")\n",
    "\n",
    "# Total number of ternary weights (+w, -w)\n",
    "totalParams = 0\n",
    "for i in layer_name:\n",
    "  print(i,(model.state_dict()[i] !=0).sum())\n",
    "  totalParams +=  (model.state_dict()[i] !=0).sum()\n",
    "    \n",
    "print(\"Total Parameters:\",totalParams, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1rWhB6O2-SWF",
    "outputId": "496e7be6-f880-466e-e3eb-e4a62996a4dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number Of Images Tested = 10000\n",
      "\n",
      "Model Test Accuracy = 0.881\n"
     ]
    }
   ],
   "source": [
    "# Model's performance on test set\n",
    "\n",
    "correct_count, all_count = 0, 0\n",
    "model.eval()\n",
    "for images,labels in test_iterator:\n",
    "      for image,label in zip(images,labels):\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            img = image.cuda()\n",
    "            lab = label.cuda()\n",
    "            img = img[None,].type('torch.cuda.FloatTensor')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_ = model(img) \n",
    "\n",
    "        pred_label = output_.argmax()\n",
    "\n",
    "        if(pred_label.item()==lab.item()):\n",
    "          correct_count += 1\n",
    "        all_count += 1\n",
    "\n",
    "print(\"Number Of Images Tested =\", all_count)\n",
    "print(\"\\nModel Test Accuracy =\", (correct_count/(all_count)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "iBPItSwJ-SWK"
   },
   "outputs": [],
   "source": [
    "# Duplicate architecture of AlexNet Model\n",
    "\n",
    "class AlexNet1(nn.Module):\n",
    "    def __init__(self, output_dim, dn_info, dn_info1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dn_info = dn_info       # Dead Neuron info\n",
    "        self.dn_info1 = dn_info1       # Dead Neuron info\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, 1), #in_channels, out_channels, kernel_size, stride, padding\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Conv2d(32, 64, 3, padding = 1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Conv2d(64, 128, 3, padding = 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Conv2d(128, 256, 3, padding = 1),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Conv2d(256, 256, 3, padding = 1),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),)\n",
    "        \n",
    "        \n",
    "        self.relu = nn.ReLU(inplace = True)\n",
    "        \n",
    "        self.dp = nn.Dropout(0.1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(256 * 6 * 6, 256, bias= True)\n",
    "        \n",
    "        # Fully connected 2\n",
    "        self.fc3 = nn.Linear(256, 256,bias= True) \n",
    "\n",
    "        # Fully connected 3\n",
    "        self.fc5 = nn.Linear(256,output_dim, bias= True) \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        h = x.view(x.size(0), -1)\n",
    "        h = self.dp(h)\n",
    "        x = self.fc1(h)\n",
    "        \n",
    "        x = self.relu(x)\n",
    "        \n",
    "        ###################################\n",
    "        \n",
    "        # Storing dead neurons indices\n",
    "        idx = torch.where(x.cpu() == 0.)[1]\n",
    "        \n",
    "        for i in idx:\n",
    "            self.dn_info[str(i.item())] += 1\n",
    "            \n",
    "        #####################################\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        ####################################\n",
    "        # Storing dead neurons indices\n",
    "        idx1 = torch.where(x.cpu() == 0.)[1]\n",
    "        \n",
    "        for j in idx1:\n",
    "            self.dn_info1[str(j.item())] += 1\n",
    "            \n",
    "        #####################################\n",
    "        \n",
    "        x =  self.fc5(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kln07LPsEfHJ",
    "outputId": "8255075a-73d6-43f5-f56d-b9a3fbdb9cf9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['features.0.weight', 'features.0.bias', 'features.1.weight', 'features.1.bias', 'features.1.running_mean', 'features.1.running_var', 'features.1.num_batches_tracked', 'features.3.weight', 'features.3.bias', 'features.6.weight', 'features.6.bias', 'features.7.weight', 'features.7.bias', 'features.7.running_mean', 'features.7.running_var', 'features.7.num_batches_tracked', 'features.9.weight', 'features.9.bias', 'features.11.weight', 'features.11.bias', 'fc1.weight', 'fc1.bias', 'fc3.weight', 'fc3.bias', 'fc5.weight', 'fc5.bias']\n"
     ]
    }
   ],
   "source": [
    "keys = list(AlexNet1(10, {}, {}).state_dict().keys())\n",
    "print(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "F1yJLAePErZ6"
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "weights_q = OrderedDict()\n",
    "\n",
    "for n in model.state_dict().keys():\n",
    "    if n in keys:\n",
    "        weights_q[n] = model.state_dict()[n].clone()\n",
    "        weights_q[n].requires_grad=False\n",
    "    \n",
    "    elif n.startswith(\"classifier\"):\n",
    "        weights_q['fc' + n[11:]] = model.state_dict()[n].clone()\n",
    "        weights_q['fc' + n[11:]].requires_grad=False\n",
    "\n",
    "state_dict = weights_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ck3A_UAXFMvA",
    "outputId": "e93bc4f7-b59d-4eab-820c-c86ab51588c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['features.0.bias', 'features.0.weight', 'features.1.bias', 'features.1.weight', 'features.1.running_mean', 'features.1.running_var', 'features.1.num_batches_tracked', 'features.3.bias', 'features.3.weight', 'features.6.bias', 'features.6.weight', 'features.7.bias', 'features.7.weight', 'features.7.running_mean', 'features.7.running_var', 'features.7.num_batches_tracked', 'features.9.bias', 'features.9.weight', 'features.11.bias', 'features.11.weight', 'fc1.bias', 'fc1.weight', 'fc3.bias', 'fc3.weight', 'fc5.bias', 'fc5.weight'])\n"
     ]
    }
   ],
   "source": [
    "print(state_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Dm34tLz5A646"
   },
   "outputs": [],
   "source": [
    "def getDeadN_info(dn_info, dn_info1, unit, state_dict, ds, nameOftheSet):\n",
    "\n",
    "  for i in range(unit):\n",
    "      dn_info[str(i)] = 0\n",
    "      dn_info1[str(i)] = 0\n",
    "\n",
    "      \n",
    "  model1 = AlexNet1(10, dn_info, dn_info1)\n",
    "  model1 = model1.cuda()\n",
    "\n",
    "  model1.load_state_dict(state_dict)\n",
    "\n",
    "  correct_count, all_count = 0, 0\n",
    "  model1.eval()\n",
    "\n",
    "  for images,labels in ds:\n",
    "        for image,label in zip(images,labels):\n",
    "\n",
    "          if torch.cuda.is_available():\n",
    "              img = image.cuda()\n",
    "              lab = label.cuda()\n",
    "              img = img[None,].type('torch.cuda.FloatTensor')\n",
    "\n",
    "          with torch.no_grad():\n",
    "              output_ = model1(img) \n",
    "\n",
    "          pred_label = output_.argmax()\n",
    "\n",
    "          if(pred_label.item()==lab.item()):\n",
    "            correct_count += 1\n",
    "          all_count += 1\n",
    "\n",
    "  print(\"Number Of Images =\", all_count)\n",
    "  print(f\"Model {nameOftheSet} Accuracy =\", (correct_count/(all_count)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U6xfQZTf-SWN",
    "outputId": "af9d0538-4bf1-4c3b-9e07-cd0ad5c56916"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number Of Images = 50000\n",
      "Model Training Accuracy = 0.84964\n",
      "Number Of Images = 10000\n",
      "Model Validation Accuracy = 0.8434\n",
      "Number Of Images = 10000\n",
      "Model Test Accuracy = 0.881\n"
     ]
    }
   ],
   "source": [
    "dn_info_train = {}\n",
    "dn_info_val = {}\n",
    "dn_info_test = {}\n",
    "\n",
    "dn_info_train1 = {}\n",
    "dn_info_val1 = {}\n",
    "dn_info_test1 = {}\n",
    "\n",
    "getDeadN_info(dn_info= dn_info_train, dn_info1= dn_info_train1, unit = 256, state_dict = state_dict, ds= train_iterator, nameOftheSet = \"Training\")\n",
    "getDeadN_info(dn_info= dn_info_val, dn_info1= dn_info_val1, unit = 256, state_dict = state_dict, ds= val_iterator, nameOftheSet = \"Validation\")\n",
    "getDeadN_info(dn_info= dn_info_test, dn_info1= dn_info_test1, unit = 256, state_dict = state_dict, ds= test_iterator, nameOftheSet = \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K4TNo4DY-SWt",
    "outputId": "a6422237-6b8c-4e51-8741-a6269c1b1a33"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10000, 10000, 50000, 10000, 10000)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_dn_val = max(dn_info_val.values())\n",
    "max_dn_test = max(dn_info_test.values())\n",
    "max_dn_train = max(dn_info_train.values())\n",
    "\n",
    "max_dn_val1 = max(dn_info_val1.values())\n",
    "max_dn_test1 = max(dn_info_test1.values())\n",
    "max_dn_train1 = max(dn_info_train1.values())\n",
    "\n",
    "max_dn_train, max_dn_val, max_dn_test, max_dn_train1, max_dn_val1, max_dn_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "8L6YTZxX-SWu"
   },
   "outputs": [],
   "source": [
    "dead_n_idx = [] \n",
    "\n",
    "for i, j in dn_info_test.items():\n",
    "  if j == max_dn_test:\n",
    "    dead_n_idx.append(i)\n",
    "\n",
    "dead_n_idx1 = [] \n",
    "\n",
    "for i, j in dn_info_test1.items():\n",
    "  if j == max_dn_test1:\n",
    "    dead_n_idx1.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-K_DTcbG-SWu",
    "outputId": "39791fbf-2904-4966-b502-bc6d97d3763c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Penultimate Layer : 193\n",
      "Output Layer : 147\n"
     ]
    }
   ],
   "source": [
    "print(\"Penultimate Layer :\",len(dead_n_idx)) # Number of neurons that are dead\n",
    "print(\"Output Layer :\",len(dead_n_idx1)) # Number of neurons that are dead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xAK4Mddws52K",
    "outputId": "e6fd65ae-ba4e-4bb8-91c4-ec1e52eba080"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1', '2', '3', '4', '6', '7', '9', '10', '11', '12', '13', '14', '15', '17', '18', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '32', '33', '35', '37', '40', '41', '42', '43', '44', '46', '47', '48', '49', '50', '52', '53', '55', '56', '57', '58', '59', '61', '62', '64', '65', '66', '69', '70', '71', '73', '75', '76', '78', '79', '80', '81', '82', '83', '84', '85', '86', '88', '89', '90', '92', '93', '95', '96', '97', '98', '99', '100', '101', '103', '104', '107', '108', '109', '110', '111', '112', '113', '114', '115', '117', '118', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '132', '133', '136', '137', '139', '140', '143', '144', '145', '146', '147', '148', '149', '150', '151', '157', '158', '159', '160', '161', '162', '163', '165', '166', '167', '168', '169', '171', '172', '173', '175', '176', '177', '179', '181', '182', '184', '185', '186', '187', '189', '192', '195', '196', '197', '198', '199', '200', '201', '203', '204', '205', '206', '207', '209', '210', '211', '212', '213', '214', '216', '217', '219', '221', '222', '223', '224', '226', '228', '229', '231', '232', '233', '234', '235', '236', '237', '238', '239', '241', '242', '243', '245', '246', '247', '248', '250', '252', '253', '254']\n"
     ]
    }
   ],
   "source": [
    "print(dead_n_idx) # Indices of neuron that are dead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i_NqP9mgGqBz",
    "outputId": "01e6b4f2-90ce-4480-9917-90cdd9eaa6ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '5', '6', '10', '12', '17', '19', '22', '24', '26', '29', '32', '33', '34', '36', '37', '38', '39', '41', '42', '43', '48', '49', '50', '51', '53', '54', '57', '59', '62', '66', '67', '68', '69', '70', '72', '73', '74', '75', '76', '78', '79', '80', '81', '82', '83', '87', '91', '92', '94', '95', '96', '98', '100', '101', '103', '104', '106', '107', '110', '111', '115', '116', '117', '118', '121', '123', '126', '128', '129', '132', '134', '135', '137', '139', '141', '144', '145', '147', '149', '150', '151', '152', '153', '158', '159', '164', '166', '168', '170', '171', '172', '173', '174', '175', '176', '178', '179', '180', '181', '182', '186', '187', '189', '191', '194', '195', '197', '199', '200', '201', '202', '204', '206', '207', '209', '212', '213', '216', '217', '218', '219', '221', '222', '223', '224', '225', '226', '229', '233', '235', '236', '237', '238', '239', '240', '241', '243', '244', '245', '246', '248', '249', '251', '252', '253', '255']\n"
     ]
    }
   ],
   "source": [
    "print(dead_n_idx1) # Indices of neuron that are dead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "PmmQ5GdzGsL4"
   },
   "outputs": [],
   "source": [
    "state_dict1 = state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xph88z2uGvUY",
    "outputId": "baf45826-5476-4917-df54-9cbc8b8907bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.0.weight hidden layer dimension torch.Size([32, 1, 3, 3])\n",
      "Unique values of weight in features.0.weight the hidden layer :  tensor([-0.9837,  0.0000,  0.9837], device='cuda:0')\n",
      "\n",
      "features.0.bias hidden layer dimension torch.Size([32])\n",
      "Unique values of weight in features.0.bias the hidden layer :  tensor([-0.9982,  0.0000,  0.9985], device='cuda:0')\n",
      "\n",
      "features.1.weight hidden layer dimension torch.Size([32])\n",
      "Unique values of weight in features.1.weight the hidden layer :  tensor([0.0000, 1.0144], device='cuda:0')\n",
      "\n",
      "features.1.bias hidden layer dimension torch.Size([32])\n",
      "Unique values of weight in features.1.bias the hidden layer :  tensor([-1.0126,  0.0000,  0.9783], device='cuda:0')\n",
      "\n",
      "features.3.weight hidden layer dimension torch.Size([64, 32, 3, 3])\n",
      "Unique values of weight in features.3.weight the hidden layer :  tensor([-1.0157,  0.0000,  1.0089], device='cuda:0')\n",
      "\n",
      "features.3.bias hidden layer dimension torch.Size([64])\n",
      "Unique values of weight in features.3.bias the hidden layer :  tensor([-1.0082,  0.0000,  1.0208], device='cuda:0')\n",
      "\n",
      "features.6.weight hidden layer dimension torch.Size([128, 64, 3, 3])\n",
      "Unique values of weight in features.6.weight the hidden layer :  tensor([-0.9646,  0.0000,  0.9646], device='cuda:0')\n",
      "\n",
      "features.6.bias hidden layer dimension torch.Size([128])\n",
      "Unique values of weight in features.6.bias the hidden layer :  tensor([-1.0022,  0.0000,  1.0010], device='cuda:0')\n",
      "\n",
      "features.7.weight hidden layer dimension torch.Size([128])\n",
      "Unique values of weight in features.7.weight the hidden layer :  tensor([0.0000, 0.9508], device='cuda:0')\n",
      "\n",
      "features.7.bias hidden layer dimension torch.Size([128])\n",
      "Unique values of weight in features.7.bias the hidden layer :  tensor([-0.9746,  0.0000], device='cuda:0')\n",
      "\n",
      "features.9.weight hidden layer dimension torch.Size([256, 128, 3, 3])\n",
      "Unique values of weight in features.9.weight the hidden layer :  tensor([-0.9833,  0.0000,  0.9534], device='cuda:0')\n",
      "\n",
      "features.9.bias hidden layer dimension torch.Size([256])\n",
      "Unique values of weight in features.9.bias the hidden layer :  tensor([-1.0154,  0.0000,  1.0084], device='cuda:0')\n",
      "\n",
      "features.11.weight hidden layer dimension torch.Size([256, 256, 3, 3])\n",
      "Unique values of weight in features.11.weight the hidden layer :  tensor([-0.9734,  0.0000,  0.9504], device='cuda:0')\n",
      "\n",
      "features.11.bias hidden layer dimension torch.Size([256])\n",
      "Unique values of weight in features.11.bias the hidden layer :  tensor([-0.9801,  0.0000,  1.0047], device='cuda:0')\n",
      "\n",
      "fc1.weight hidden layer dimension torch.Size([256, 9216])\n",
      "Unique values of weight in fc1.weight the hidden layer :  tensor([-0.9753,  0.0000,  0.9532], device='cuda:0')\n",
      "\n",
      "fc1.bias hidden layer dimension torch.Size([256])\n",
      "Unique values of weight in fc1.bias the hidden layer :  tensor([-1.0000,  0.0000,  0.9955], device='cuda:0')\n",
      "\n",
      "fc3.weight hidden layer dimension torch.Size([256, 256])\n",
      "Unique values of weight in fc3.weight the hidden layer :  tensor([-0.9861,  0.0000,  0.9487], device='cuda:0')\n",
      "\n",
      "fc3.bias hidden layer dimension torch.Size([256])\n",
      "Unique values of weight in fc3.bias the hidden layer :  tensor([-1.0168,  0.0000,  0.9844], device='cuda:0')\n",
      "\n",
      "fc5.weight hidden layer dimension torch.Size([10, 256])\n",
      "Unique values of weight in fc5.weight the hidden layer :  tensor([-1.0486,  0.0000,  0.9514], device='cuda:0')\n",
      "\n",
      "fc5.bias hidden layer dimension torch.Size([10])\n",
      "Unique values of weight in fc5.bias the hidden layer :  tensor([-0.9897,  0.0000,  0.9946], device='cuda:0')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For each layer, model's ternary weights\n",
    "\n",
    "layer_distinct_weights = {}\n",
    "\n",
    "for i in keys:\n",
    "  if (\"mean\" in i) | (\"var\" in i) | ('batches' in i):\n",
    "    continue\n",
    "  else:\n",
    "    imd = torch.unique(state_dict[i])\n",
    "    print(i+ ' hidden layer dimension', state_dict[i].shape)\n",
    "    print(\"Unique values of weight in \"+ i+ \" the hidden layer : \", imd)\n",
    "    layer_distinct_weights[i] = imd.cpu().numpy().tolist()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x7M72cWcUMtr",
    "outputId": "199c2639-b79e-4771-9cbf-e0a1d12e78dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.0.weight torch.Size([32, 1, 3, 3])\n",
      "features.0.bias torch.Size([32])\n",
      "features.1.weight torch.Size([32])\n",
      "features.1.bias torch.Size([32])\n",
      "features.1.running_mean torch.Size([32])\n",
      "features.1.running_var torch.Size([32])\n",
      "features.1.num_batches_tracked torch.Size([])\n",
      "features.3.weight torch.Size([64, 32, 3, 3])\n",
      "features.3.bias torch.Size([64])\n",
      "features.6.weight torch.Size([128, 64, 3, 3])\n",
      "features.6.bias torch.Size([128])\n",
      "features.7.weight torch.Size([128])\n",
      "features.7.bias torch.Size([128])\n",
      "features.7.running_mean torch.Size([128])\n",
      "features.7.running_var torch.Size([128])\n",
      "features.7.num_batches_tracked torch.Size([])\n",
      "features.9.weight torch.Size([256, 128, 3, 3])\n",
      "features.9.bias torch.Size([256])\n",
      "features.11.weight torch.Size([256, 256, 3, 3])\n",
      "features.11.bias torch.Size([256])\n",
      "fc1.weight torch.Size([256, 9216])\n",
      "fc1.bias torch.Size([256])\n",
      "fc3.weight torch.Size([256, 256])\n",
      "fc3.bias torch.Size([256])\n",
      "fc5.weight torch.Size([10, 256])\n",
      "fc5.bias torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for i in keys:\n",
    "  print(i, state_dict[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PpWoItoXGzW_",
    "outputId": "f56c579a-abfa-4042-ef2f-ec1e040493bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc3.weight\n",
      "-0.9861384630203247 0.0 0.9487199783325195\n",
      "fc5.weight\n",
      "-1.048563003540039 0.0 0.9514178037643433\n",
      "Total dead transitions for the layer : 0\n"
     ]
    }
   ],
   "source": [
    "total_trans = 0\n",
    "layers_name = ['fc3.weight', 'fc5.weight']\n",
    "\n",
    "n_idx = [dead_n_idx, dead_n_idx1]\n",
    "\n",
    "for ix,l in enumerate(layers_name):\n",
    "  print(l)\n",
    "  z = state_dict1[l]\n",
    "\n",
    "  if len(layer_distinct_weights[l]) > 2 :\n",
    "    w_neg, w_0, w_pos =  layer_distinct_weights[l]\n",
    "    print(w_neg, w_0, w_pos)\n",
    "  else:\n",
    "    w_neg, w_pos = layer_distinct_weights[l]\n",
    "    print(w_neg, w_pos)\n",
    "\n",
    "  for idx in n_idx[ix]:\n",
    "      if 'bias' in l:\n",
    "        imd = z[eval(idx)]\n",
    "      else:\n",
    "        imd = z[:,eval(idx)]\n",
    "\n",
    "      trans = torch.where(imd == w_neg)[0].nelement() + torch.where(imd == w_pos)[0].nelement()\n",
    "      total_trans += trans *2\n",
    "    \n",
    "print(\"Total dead transitions for the layer :\", total_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CmrFDwTBIa7a"
   },
   "outputs": [],
   "source": [
    "# Fault coverage that we have obtained from main file : 8410/8689"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M8IONyquIjmL",
    "outputId": "f335a5d6-a813-4a19-f1b5-e7155f90eb46"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9678904361836805"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding 0 transition to the numerator will make the net fault coverage\n",
    "(8410 + 0)/ 8689"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CxRMcZPjIqhk"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DeadNeurons.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
